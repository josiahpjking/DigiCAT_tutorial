[
  {
    "objectID": "01_introcf.html",
    "href": "01_introcf.html",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "",
    "text": "Counterfactual analysis is a set of techniques for trying to understand the impact of a treatment. It does so by comparing people who experienced that treatment to a hypothetical ‘counterfactual’ version of themselves who did not. Since in the real world, people either do or not experience a treatment, we cannot literally compare the same person under the scenario of experiencing versus not experiencing a treatment. Instead controls (who did not experience the treatment) who are very similar to the treated people in all important respects are used to stand in for the counterfactual. By ‘very similar in all important respects’ we mean that although they did not experience the treatment, they had a very similar propensity to or ‘similar chances of’ experiencing the treatment’ as a person who did experience the treatment. This propensity is estimated based on their levels of ‘confounding factors’ I.e., factors that impact both the likelihood of receiving the treatment and the outcome of interest. We can compare treated and control units on an outcome of interest (e.g., depression) and if those outcomes differ we can be more sure that it is due to the treatment, rather than any confounding factors. As such, counterfactual analysis can support causal inference in a way that models that don’t take into account confounding factors cannot. As we discuss in these tutorials, there are several methods by which propensities can be estimated and used to support causal inference in observational data.\nAs an example of where counterfactual analysis can be useful, we might ask whether social media has a negative impact on adolescents’ mental health. We might find an association between social media use and depression in adolescents. However, we can’t be sure yet that this association is not due to confounding. For example, maybe adolescents with poorer mental health tend to turn to social media more (and are also more likely to continue to have poorer mental health ,given the stability of mental health over time). Or maybe there are other confounding factors, like attention deficit hyperactivity disorder (ADHD) that might lead people to use more social media and separately increase their risk of depression. If the set of potential confounding factors in this association can be identified and measured they could be used in a counterfactual analysis to see if there is still an association between social media use and depression after accounting for confounding. People with similar propensities to use social media a lot but who differ in their actual use could be compared. If those who use social media more show higher depression levels, we can be more sure that this is actually due to the social media use, rather than other confounding factors. We will use the example of social media use and mental health as an example throughout these tutorials as social media use was identified by our young person advisory group as a factor that they thought it particularly important to explore in research on young people’s mental health."
  },
  {
    "objectID": "01_introcf.html#what-is-counterfactual-analysis",
    "href": "01_introcf.html#what-is-counterfactual-analysis",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "",
    "text": "Counterfactual analysis is a set of techniques for trying to understand the impact of a treatment. It does so by comparing people who experienced that treatment to a hypothetical ‘counterfactual’ version of themselves who did not. Since in the real world, people either do or not experience a treatment, we cannot literally compare the same person under the scenario of experiencing versus not experiencing a treatment. Instead controls (who did not experience the treatment) who are very similar to the treated people in all important respects are used to stand in for the counterfactual. By ‘very similar in all important respects’ we mean that although they did not experience the treatment, they had a very similar propensity to or ‘similar chances of’ experiencing the treatment’ as a person who did experience the treatment. This propensity is estimated based on their levels of ‘confounding factors’ I.e., factors that impact both the likelihood of receiving the treatment and the outcome of interest. We can compare treated and control units on an outcome of interest (e.g., depression) and if those outcomes differ we can be more sure that it is due to the treatment, rather than any confounding factors. As such, counterfactual analysis can support causal inference in a way that models that don’t take into account confounding factors cannot. As we discuss in these tutorials, there are several methods by which propensities can be estimated and used to support causal inference in observational data.\nAs an example of where counterfactual analysis can be useful, we might ask whether social media has a negative impact on adolescents’ mental health. We might find an association between social media use and depression in adolescents. However, we can’t be sure yet that this association is not due to confounding. For example, maybe adolescents with poorer mental health tend to turn to social media more (and are also more likely to continue to have poorer mental health ,given the stability of mental health over time). Or maybe there are other confounding factors, like attention deficit hyperactivity disorder (ADHD) that might lead people to use more social media and separately increase their risk of depression. If the set of potential confounding factors in this association can be identified and measured they could be used in a counterfactual analysis to see if there is still an association between social media use and depression after accounting for confounding. People with similar propensities to use social media a lot but who differ in their actual use could be compared. If those who use social media more show higher depression levels, we can be more sure that this is actually due to the social media use, rather than other confounding factors. We will use the example of social media use and mental health as an example throughout these tutorials as social media use was identified by our young person advisory group as a factor that they thought it particularly important to explore in research on young people’s mental health."
  },
  {
    "objectID": "01_introcf.html#what-kind-of-research-questions-can-be-answered-with-counterfactual-analysis",
    "href": "01_introcf.html#what-kind-of-research-questions-can-be-answered-with-counterfactual-analysis",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "What kind of research questions can be answered with counterfactual analysis?",
    "text": "What kind of research questions can be answered with counterfactual analysis?\nCounterfactual analysis can be used to address research questions concerned with the causal impact of some candidate treatment or exposure on some outcome variable of interest. Usually this is in observational data where it is not possible to control whether people experience a treatment or not. It is particularly valuable when it is suspected that people do not randomly experience a treatment or not and this might also be related to the outcome of interest (e.g., we suspect that certain adolescents who use social media more might differ from those who use it less in ways that also increase their risk of depression. Counterfactual analysis needs three main things:\n\nA treatment variable (e.g., social media use)\n\nAn outcome variable (e.g., depression)\n\nConfounders or ’matching variables (e.g., ADHD, prior depression, sex/gender, socioeconomic status, physical health etc.)\n\nOther than the social media use example above, examples of research questions that could be asked using counterfactual analysis include:\n\nWhat is the impact of reading for pleasure on mental health?\nDoes school exclusion lead to poorer labour market outcomes?\nDo more positive relationships with teachers lead to fewer behavioural problems in adolescence?\n\nIn fact, as DigiCAT was originally developed for mental health researchers we asked young people from our lived experience advisory groups what mental health research questions they think would be most interesting to investigate with counterfactual analysis. Here are some examples of the questions they felt should be a priority for mental health researchers:\n\nWhat is the impact of social media use on mental health?\nWhat is the impact of poorer sleep on mental health?\nWhat is the impact of poor peer relationships on mental health?\nWhat is the impact of spending time in nature on mental health?\nWhat is the impact of physical activity on mental health?"
  },
  {
    "objectID": "01_introcf.html#why-not-just-adjust-for-covariates-in-regression",
    "href": "01_introcf.html#why-not-just-adjust-for-covariates-in-regression",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "Why not just adjust for covariates in regression?",
    "text": "Why not just adjust for covariates in regression?\nPerhaps the most widely used approach to attempting to deal with confounding is to adjust for potential confounding within a regression or ANOVA-type model. This approach should be described as ‘conditional adjustment’. However, counterfactual analysis provides a more principled method that places causal inference at the fore. Importantly, it includes steps to check and ensure good covariate balance has been achieved. This is not guaranteed and may fail in traditional covariate adjustment approaches, leading to biases when trying to understand the causal effect of a treatment without any warning signs for the user. Further, counterfactual analysis provides a way of more clearly defining the target population about which inferences are being made. It can also readily incorporate a very large number of covariates. None of this comes at the cost of Interpretability when using methods like propensity matching or weighting. In DigiCAT in particular, both methods use a linear regression for the outcome model so the interpretation of the treatment effect is no more complicated than in a traditional regression model. ## What counterfactual analysis approaches are offered in this tool?\nDigiCAT currently offers several broad approaches to counterfactual analysis: propensity score matching for binary (traditional propensity score analysis) and ordinal treatment variables (non-bipartite optimal matching) and propensity score weighting for binary variables (inverse propensity of treatment weighting; IPTW). Propensity score matching for binary treatments is based on matching treated and control units with similar propensity scores to achieve balance between treated and control groups. Propensity weighting for binary treatments also uses a propensity score but transforms it into a weight that is used in a weighted regression. The weighted regression up-weights some cases and down-weights others to achieve balance between the treated and control groups. Non-bipartite propensity scoring estimates a propensity score based on treating the treatment variable as ordinal and then matches cases that have similar propensity scores but different treatment levels. We also have several features that are planned for incorporation in the future.\nWe are continuing to develop DigiCAT to make more approaches available. These are listed under features in development. Feel free to get in touch uoe_digicat-group@uoe.onmicrosoft.com if you have suggestions for approaches you would like to see within the tool."
  },
  {
    "objectID": "02_howto_digicat.html",
    "href": "02_howto_digicat.html",
    "title": "1. Using DigiCAT",
    "section": "",
    "text": "Counterfactual analysis provides a set of tools that can help us understand what the active ingredients are in mental health. While the gold standard for doing so would be something like a randomised controlled trial (RCT), RCTs are not always possible and when they are, they are expensive and challenging to do well. Their highly controlled nature also means that they may lack ecological validity so that results from RCTs might not apply very well in the real world.\nAs such there is a lot of value in being able to get an idea of what might impact mental health from observational data (I.e., non-experimental data). In fact, there is a wealth of pre-existing mental health data ready to be used in this way. Some examples of such datasets can be found in, for example, this catalogue of mental health datasets: https://www.cataloguementalhealth.ac.uk/\nHowever, we noticed that counterfactual analysis is not as widely used in mental health research as one might expect. There may be a number of reasons for this but we think a major one is that counterfactual analysis might not be very accessible for mental health researchers. It would not typically be part of training and it can be technically quite complex, which may limit researcher’s ability to use it. There’s also a lack of accessible tools for counterfactual analysis. Most tools require coding skills and while some web applications exist they don’t do everything. In particular, their functionality tends to be narrowly focused on a particular type of analysis and they tend not be able to deal with the ‘real world’ features of data like the presence of missingness or complex survey data or non-binary treatments. These are things we wanted to address with DigiCAT.\nAnother reason we made DigiCAT is we noticed that it took rather a long time to code counterfactual analyses using existing packages and things got complicated when we had to consider things like missingness, clustering, large numbers of confounders etc. By pulling together all the code from common ‘journeys’ and creating an easy ‘point and click’ interface we also aim to speed up the implementation of counterfactual analysis even for expert users adept in the technique in existing softwares like R."
  },
  {
    "objectID": "02_howto_digicat.html#why-develop-digicat",
    "href": "02_howto_digicat.html#why-develop-digicat",
    "title": "1. Using DigiCAT",
    "section": "",
    "text": "Counterfactual analysis provides a set of tools that can help us understand what the active ingredients are in mental health. While the gold standard for doing so would be something like a randomised controlled trial (RCT), RCTs are not always possible and when they are, they are expensive and challenging to do well. Their highly controlled nature also means that they may lack ecological validity so that results from RCTs might not apply very well in the real world.\nAs such there is a lot of value in being able to get an idea of what might impact mental health from observational data (I.e., non-experimental data). In fact, there is a wealth of pre-existing mental health data ready to be used in this way. Some examples of such datasets can be found in, for example, this catalogue of mental health datasets: https://www.cataloguementalhealth.ac.uk/\nHowever, we noticed that counterfactual analysis is not as widely used in mental health research as one might expect. There may be a number of reasons for this but we think a major one is that counterfactual analysis might not be very accessible for mental health researchers. It would not typically be part of training and it can be technically quite complex, which may limit researcher’s ability to use it. There’s also a lack of accessible tools for counterfactual analysis. Most tools require coding skills and while some web applications exist they don’t do everything. In particular, their functionality tends to be narrowly focused on a particular type of analysis and they tend not be able to deal with the ‘real world’ features of data like the presence of missingness or complex survey data or non-binary treatments. These are things we wanted to address with DigiCAT.\nAnother reason we made DigiCAT is we noticed that it took rather a long time to code counterfactual analyses using existing packages and things got complicated when we had to consider things like missingness, clustering, large numbers of confounders etc. By pulling together all the code from common ‘journeys’ and creating an easy ‘point and click’ interface we also aim to speed up the implementation of counterfactual analysis even for expert users adept in the technique in existing softwares like R."
  },
  {
    "objectID": "02_howto_digicat.html#our-approach",
    "href": "02_howto_digicat.html#our-approach",
    "title": "1. Using DigiCAT",
    "section": "Our approach",
    "text": "Our approach\nDigiCAT was built with FAIR principles: https://www.go-fair.org/fair-principles/ in mind and is therefore built in open source software R and Shiny. All code is available at the DigiCAT github and you can also download the code for your specific analysis by clicking ‘Download R script’ at the end of the workflow. This is also useful if you would like to build on the analyses offered in DigiCAT: it provides a script that you can adapt, extend, and otherwise customise to your needs. DigiCAT development also took a user centred approach and many features have been adapted based on the helpful input we received from our user engagements. It is also informed by lived experience expert input and you can read more about below about how you might draw on lived experience expertise in planning, interpreting and disseminating your analyses using the tool."
  },
  {
    "objectID": "02_howto_digicat.html#how-to-get-digicat",
    "href": "02_howto_digicat.html#how-to-get-digicat",
    "title": "1. Using DigiCAT",
    "section": "How to get DigiCAT",
    "text": "How to get DigiCAT\nThere are two ways to use DigiCAT. The first is to download it and use it on your local computer. This is recommended if you need to avoid uploading your data to a server or if you have a very large dataset or very computationally intensive analyses. The second way to use it is via the web application. The two versions are mostly very similar but the web application has a few more limits. These limits are to help ensure data security and to limit very computationally demanding analyses (that would take a long time to run). These types of analyses are better to do with the package version.\n\n\n\n\n\n\nR Package\n\n\n\nTo use DigiCAT on your local computer you should have a recent version of R installed. Within R you can then enter the following lines to install and then run DigiCAT. Once it is installed you only need to run the third line to launch it each time.\n\ninstall.packages(\"remotes\") \nremotes::install_github(\"uoe-digicat/DigiCAT\")\nDigiCAT::run_DigiCAT()\n\n\n\n\n\n\n\n\n\nWeb Version\n\n\n\nThe web version of DigiCAT is available at https://digicatapp.shinyapps.io/DigiCAT. For the moment we have disabled data upload until we move it over to a new server. However, you can use the example dataset pre-loaded with the tool to explore its features."
  },
  {
    "objectID": "02_howto_digicat.html#how-to-use-digicat",
    "href": "02_howto_digicat.html#how-to-use-digicat",
    "title": "1. Using DigiCAT",
    "section": "How to use DigiCAT",
    "text": "How to use DigiCAT\nBoth the downloadable and package version of DigiCAT guide you through a journey which begins with data upload (or loading of example datasets) then proceeds through the selection of a counterfactual analysis approach, a balancing stage, and an outcome model stage. Each of these is explained in turn below and more details of the specific methods offered are provided in later sections.\n\nData upload\nThe downloadable version of DigiCAT allows users to upload data. This data should be in .csv format and include variable headers. All the data should be in ‘numeric’ form meaning that only numbers can be used to represent variable values. This means that variable labels should be recoded as numbers, e.g., ‘yes = 1; no=0’. Missing values should be coded as ‘NA’ which is the convention that R, the tool that powers DigiCAT uses to represent missing values.\nOne a file has been selected for upload you are then able to flag any categorical variables. This is to ensure that they are properly treated as categorical. You also select your outcome variable (e.g., ‘anxiety’, your treatment variable (e.g., ‘social media use’) and your matching variables (e.g., ‘gender, socioeconomic status’ etc.). You can also select any additional covariates that you want to include in your outcome model but that you don’t want to use as matching variables. These might be selected if you want to estimate a treatment effect over and above the effects of other influences on mental health (e.g., the effect of social media use after adjusting for the effects of physical activity).\nFinally, DigiCAT has been built to handle complex survey data which is commonly seem in large-scale mental health studies. For datasets like this you can select a survey weight or non-response weight variable. If only a survey weight variable is available you can select ‘select if data includes survey weight’. If a non-response or attrition weight is available then you can additionally select ‘select if survey weights above compensate for non-response’. Complex survey datasets also commonly include clustering and stratification variables which can also be selected at the data upload stage.,\nOnce the various variables have been selected only these will be carried forward for analysis. After uploading the data and selecting your variables you can then click ‘validate data’ and DigiCAT will perform some checks to make sure the data are all in the right format no issues with the analysis are anticipated.\n\n\nApproach\nAfter uploading the data and validating it you can then proceed to the ‘Approach’ page which is where you select your counterfactual analysis and missingness approaches. Note that on this page you will only be shown options that are applicable to the data you uploaded/selected. For example, if your treatment variable is binary you will only be shown counterfactual analysis options for binary treatments. Different options are shown for ordinal treatment variables. Similarly, you will only be shown non-response weighting as a missingness option if your variable selection included a non-response weight variable. Some approaches have more options than others. For example, for propensity score analysis for binary treatments it is relevant to choose a matching algorithm and matching ratio whereas for non-bipartite propensity matching (for ordinal treatments) these parameters are fixed (always optimal matching with a 1:1 ratio).\n\n\nBalancing\nAfter choosing a balancing approach and running the ‘balancing model’ some interim output will be shown that can be used to assess the quality of the balancing. Details of this output for specific methods are provided below.\n\n\nOutcome\nThe outcome model stage uses the output from the balancing stage and fits a model to estimate the treatment effect. After selecting and running a model, the output tab provides the key information needed to see if the candidate treatment variable appears to have an impact on the outcome. An effect estimate, standard error, and p-value. For some methods marginal effects are also provided (see ‘marginal effects’ for more details). At this point you can also download the R script for your analysis."
  },
  {
    "objectID": "02_howto_digicat.html#example-dataset",
    "href": "02_howto_digicat.html#example-dataset",
    "title": "1. Using DigiCAT",
    "section": "Example Dataset",
    "text": "Example Dataset\nDigiCAT includes an example synthetic dataset which is based on the example here: TODO LINK which examined the effects of reading engagement on mental health. This can be used to help get to grips with counterfactual analysis, for teaching, or just to explore the features of DigiCAT. It includes a set of pre-selected matching, treatment and outcome variables. The treatment variable is reading engagement at age 15 and is offered as both a binary (‘Reading_age15’) and ordinal variable (‘ReadingO_age15) to allow different types of analyses to be explored. The outcome variable (’Anxiety_age17’) is a continuous measure of anxiety at age 17. The matching variables are a set of pre-treatment variables (measured at age 13 or before) that could confound the treatment-outcome association and should, therefore, be matched on. All categorical variables are pre-selected as such. The dataset also includes some missing data to allow exploration of the different missing data options. In the future we also plan to include datasets with complex survey features and other relevant features to allow all tool features to be explored with the example dataset as they are added.\nYou can use the example dataset by selecting ‘Load example data’ in the Upload data page of DigiCAT. The ‘Data’ tab on the right-hand panel will then show you the data values, allowing you to inspect the dataset and get a feel for it. The data dictionary can be seen in Table 1 below. As per data upload requirements the file is in .csv format and codes missing values as ‘NA’.\n\n\n\n\n\n\nTable 1:  Data Dictionary for DigiCAT example data \n  \n    \n    \n      Variable.label\n      Description\n      Type\n      Values\n    \n  \n  \n    Gender\nParticipant gender\nNominal\n1 = male, 2 = female*\n    Anxiety_age17\nAnxiety levels at age 17 as the average of a set of 10 anxiety items measured on a 4-point scale\nContinuous\n1 = lowest to 4 = highest\n    Reading_age15\nA single item indicating whether a participant reads more or less than once a week at age 15\nBinary\n0 = no, 1 = yes\n    ReadingO_age15\nA single item indicating the extent to which participants' read at age 15\nOrdinal\n1 = least to 4 = most\n    AnxietyDepression_age13\nA combined measure of pre-treatment anxiety and depression levels at age 13. The average of 20 items on a 4-point scale\nContinuous\n1 = lowest to 4 = highest\n    Trust_age13\nA measure of pre-treatment generalised trust (a wellbeing measure). The average of 5 items on a 4-point scale\nContinuous\n1 = lowest to 4 = highest\n    SelfControl_age13\nA measure of pre-treatment self-control. The average of 5 items on a 4-point scale\nContinuous\n1 = lowest to 4 = highest\n    SubstanceUse1_age13\n\nBinary\n0 = no, 1 = yes\n    SubstanceUse2_age13\nA measure of pre-treatment cannabis use (whether the participant uses cannabis or not)\nBinary\n0 = no, 1 = yes\n    SubstanceUse3_age13\nA measure of pre-treatment alcohol use (whether the participant uses alcohol or not)\nBinary\n0 = no, 1 = yes\n    SubstanceUse4_age13\nA measure of pre-treatment hard drug (whether the participant uses hard drugs or not)\nBinary\n0 = no, 1 = yes\n    TeacherBond_age13\nA measure of pre-treatment positive bond with teachers. The average of 4 items measured on a 4-point scale.\nContinuous\n1 = lowest to 4 = highest\n    ClassBond_age13\nA measure of pre-treatment positive bond with classmates. The average of 4 items measured on a 4-point scale.\nContinuous\n1 = lowest to 4 = highest\n    SchoolDifficulties_age13\nA measure of pre-treatment school difficulties. The average of 4 items measured on a 4-point scale.\nContinuous\n1 = lowest to 4 = highest\n    Achievements1_age13\nA measure of pre-treatment academic achievement in standardised tests of Science\nOrdinal\n1 = lowest to 5 = highest\n    Achievements2_age13\nA measure of pre-treatment academic achievement in standardised tests of Maths\nOrdinal\n1 = lowest to 5 = highest\n    Achievements3_age13\nA measure of pre-treatment academic achievement in standardised tests of English\nOrdinal\n1 = lowest to 5 = highest\n    K5_nSBQ_REAGGR\nA measure of pre-treatment (age 13) reactive aggression. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_nSBQ_PHYSAGGR\nA measure of pre-treatment (age 13) physical aggression. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_nSBQ_PROAGGR\nA measure of pre-treatment (age 13) proactive aggression. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_nSBQ_INDAGGR\nA measure of pre-treatment (age 13) proactive aggression. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_nSBQ_PROSO\nA measure of pre-treatment (age 13) prosociality. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_DevVar19\nA measure of pre-treatment (age 13) deviant behaviour. A variety score combining 5 items on a 5-point scale (0-4)\nContinuous\n1 = lowest to 4 = highest\n    K5_involv\nA measure of pre-treatment (age 13) parental involvement. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_ResilAdult\nA measure of pre-treatment (age 13) social support from adults. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_ResilFriends\nA measure of pre-treatment (age 13) social support from friends. The average of 10 items with a 4-point response scale\nContinuous\n1 = lowest to 4 = highest\n    K5_BullVict4\nA measure of pre-treatment (age 13) bullying victimisation. The average of 10 items with a 5-point response scale\nContinuous\n1 = lowest to 4 = highest\n  \n  \n  \n\n\n\n\n\n\\(*\\)Note: this does not imply that we consider gender to be binary: no other genders than male or female happened to be reported in this sample."
  },
  {
    "objectID": "02_howto_digicat.html#counterfactual-analyses-informed-by-lived-experience-experts",
    "href": "02_howto_digicat.html#counterfactual-analyses-informed-by-lived-experience-experts",
    "title": "1. Using DigiCAT",
    "section": "Counterfactual Analyses informed by lived experience experts",
    "text": "Counterfactual Analyses informed by lived experience experts\n\nWhy should the research use Lived Experience expertise ?\nIncluding LE experiences alongside this tool can be a useful reminder of the subjective nature of mental health. It is also a direct way to strengthen your research. Increasingly, co-production and PPI (patient and public involvement) is required for funding. This should be taken seriously, rather than treated as an add-on or in a tokenistic way.\nThe inclusion of lived experience voices and feedback can also help with the practical application of the research: how does it relate to real experiences? This can help you to hone in on practically relevant research questions, and facilitate clinical validity of the interpretation of research findings. If applicable, this can also help with assessing the likely efficacy in real world application of the findings.\nThe effects are not just one sided, as there can be positive impacts for lived experience collaborators. This can include empowerment, purpose, acquisition of new skills and knowledge, and having a role in changing and improving mental health research and interventions.\n\n\nHow should the research use Lived Experience expertise?\nThere is not one single definition of lived experience, and so it can come in many forms. It might include those who have previously or continue to experience mental illness difficulties, but it can also include family members of those who have experienced it.\nThose with lived experience may already be working in the academy, so it is also fruitful to engage with colleagues (across disciplines) who may bring in a unique lived experience perspective.\nLived Experience experts can help to create or refine research questions. This can include the identification of relevant factors to explore. For example, in exploring active ingredients of mental health, a young person’s advisory group were asked to rank their importance, explain reasons, and add other possible influences. This exercise demonstrated that ‘social media’ was a more important factor than initially expected by the researchers. Young people also highlighted the importance of ‘entertainment’ in general as beneficial for well-being. They compared reading books to watching movies and noted that: “Umm, the thing between movies and books is books are generally better for you because it exposes you to better vocabulary and staring at a screen for too long isn’t the best thing to do. So, it is just that entertaining thing that can really benefit you.” Answers like these may help researchers refining their focus on the important ingredients for young people’s mental health.\nThe inclusion of those with lived experience can help researchers to understand different perspectives and more fully understand key concepts. For example, further discussion in our advisory group emphasised that ‘social media’ is not homogenous but takes multiple forms, and therefore can have different impacts. The presence of advertising, for example, and how media is circulated via sites and apps may have significant negative influence on mental health. On the other hand, using it to keep in touch with friends, particularly when no other options are available due to distance or illness, may have a positive impact on mental health. As one young person pointed out relying on social media to keep in touch with friends “during COVID helped keep everybody sane”. However, they added that since then “We have not recovered from that.” Referring to social media use post covid they added: “It has worsened. People meeting up has gone down a lot or people are less willing to meet up because they’re, like, we could just text.” They pointed out that not engaging in activities outside of social media may have a negative impact on young people’s wellbeing. Further highlighting the complexities of ‘social media’ use and its impact. Similarly, when the potential impact of reading on young people’s mental health was discussed, one young person highlighted that: “some books can be more influential than others to different people, so it doesn’t matter how much you read. It matters what you read.”, again adding nuance. These examples suggest that exploring use of social media or reading would have been misleading without understanding why, how and when the young people thought these were important.\nLived experience experts can also assist in the interpretation of data. As researchers (and individuals) we come to what we do with our own set of biases and expectations, so it is therefore a good idea to run our interpretations by third parties with lived experience in the area we are wishing to inform.\nSuch consultation can also assist in the presentation of data, and co-production of research findings. In addition to publishing in scientific journals it is important to present the findings in ways that are easy to understand and utilise for those for whom they are most practically relevant; that is service users and generally those with lived experience. Such communications are best produced in collaboration with them.\nLived experience experts should also be consulted on the dissemination and use of data and findings. Collaboration with lived experience experts increases credibility and practical reliability and applications of research findings.\nWhen they were asked whether it is important to engage youths with lived experience in research, our young experts told us that it is crucial to engage them in all stages of it. One young person stated: “Adults were kids once as well, but I think, feel that since social media is involved that it’s changed a lot. So, they still had the same problems, but now there are also different things and those things could be changed to make it better.” They suggested that this could be done via online advisory groups, dedicated apps that would gather young people’s input or surveys."
  },
  {
    "objectID": "03_choosecf.html",
    "href": "03_choosecf.html",
    "title": "3. Choosing a Method",
    "section": "",
    "text": "DigiCAT offers counterfactual analysis for different types of treatment variables. Binary treatment variables (where people either experience a treatment or not) can be analysed using propensity matching or weighting (IPTW). Ordinal treatment variables (where people might experience a treatment to different degrees) can be analysed using optimal non-bipartite propensity matching. DigiCAT will try to detect what type of treatment variable you have and if it has only two categories it will assume it is binary. If it has 3-5 categories it will assume it is ordinal."
  },
  {
    "objectID": "03_choosecf.html#ordinal-vs-binary-treatments",
    "href": "03_choosecf.html#ordinal-vs-binary-treatments",
    "title": "3. Choosing a Method",
    "section": "",
    "text": "DigiCAT offers counterfactual analysis for different types of treatment variables. Binary treatment variables (where people either experience a treatment or not) can be analysed using propensity matching or weighting (IPTW). Ordinal treatment variables (where people might experience a treatment to different degrees) can be analysed using optimal non-bipartite propensity matching. DigiCAT will try to detect what type of treatment variable you have and if it has only two categories it will assume it is binary. If it has 3-5 categories it will assume it is ordinal."
  },
  {
    "objectID": "03_choosecf.html#matching-vs-weighting",
    "href": "03_choosecf.html#matching-vs-weighting",
    "title": "3. Choosing a Method",
    "section": "Matching vs Weighting",
    "text": "Matching vs Weighting\nMatching and weighting approaches are both available for binary variables. Matching can be used to estimate the ATT and weighting can be used in DigiCAT to estimate the ATE. As such, your research question (and whether it concerns the effect of a treatment in the treated or whole population) should be your primary guide your choice of matching versus weighting. There are; however, also some other differences between matching and weighting that may be considered. For example, weighting uses the whole sample, whereas matching (depending on which method is used) might result in some unmatched cases being unused. Weighting can, therefore, have advantages when the treatment is rare or the overall sample size is small. More details on these methods are provided below.\nNote that though DigiCAT implements weighting and matching, reflecting the most common ways I which propensity scores are used, these are not the only way they can be used in counterfactual analysis. For example, stratification on the propensity score is also sometimes used and sometimes (though we do not necessarily recommend it), the propensity score is used as a covariate in a regression as form of counterfactual analysis. DigiCAT will in the future also offer the option to save out estimated propensity scores so that they can be used in other ways. This also allows the scores to be used in more complex analyses that are not offered in DigiCAT, for example, using them in a structural equation model. We are grateful to the member of our user group who suggested adding this feature."
  },
  {
    "objectID": "03_choosecf.html#complete-case-versus-multiple-imputation-versus-weighting-for-missing-data",
    "href": "03_choosecf.html#complete-case-versus-multiple-imputation-versus-weighting-for-missing-data",
    "title": "3. Choosing a Method",
    "section": "Complete case versus multiple imputation versus weighting for missing data",
    "text": "Complete case versus multiple imputation versus weighting for missing data\nThere are several options for dealing with missing data available in DigiCAT: complete case analysis, multiple imputation, and weighting. Complete case analysis should be selected when you have no missing data. It is also justifiable to use it if you are confident that there is no relation between missingness and the variables you have in your model or between the missingness and the missing values of the outcome variable. Weighting is a good option for dealing with case-level missing data, i.e., some people are completely missing at a time-point. This could happen if you have longitudinal data with your matching, treatment, and outcome variables measured at sequential time-points. If you have an attrition weight for the last time-point you could use it with missingness weighting to deal with any non-random drop-out between your baseline and outcome point. Finally, multiple imputation can be used to deal with case and variable-level missingness. However, it can take a long time to run so users with large datasets and large numbers of matching variables and/or additional covariates should be aware of this. More details on the missingness methods are provided below."
  },
  {
    "objectID": "03_choosecf.html#the-benefits-of-sensitivity-analyses",
    "href": "03_choosecf.html#the-benefits-of-sensitivity-analyses",
    "title": "3. Choosing a Method",
    "section": "The benefits of sensitivity analyses",
    "text": "The benefits of sensitivity analyses\nThere are a lot of subjective decision points in counterfactual analysis, such as which matching variables to include, which additional covariates to include (if any), which missingness approach to use for missing data and whether to match or weight by a propensity score. While there have been a lot of advancements in recent years in our understanding of what best practice for implementing counterfactual analysis looks like, it is not always the case that there is one ‘best approach’. Sometimes it depends. Sometimes it’s still debated or there isn’t a lot of evidence to guide a decision. As such, we recommend where relevant using multiple different approaches and checking whether you get similar results with different approaches. If you do, this helps increase confidence in your findings. If your results differ a lot across approaches then the nature of the differences might be illuminating. For example, if you find that PSM is non-significant but IPTW is significant this may be because the latter uses more of the data (e.g., if the treatment and control groups are very different). Provided IPTW successfully balances the groups, it could suggest that IPTW is more suitable for these data.\n\n\n\n\n\n\nTechnical background to counterfactual analysis\n\n\n\n\n\nThe potential outcomes framework Counterfactual analysis is based on what is known as the ‘potential outcomes’ framework which was formalised by Rubin. It is based on the notion that for each unit exposed to a treatment W \\((W_i = 1)\\) there is a potential outcome \\((W_i = 0)\\) where they weren’t exposed to that treatment. It is assumed that each unit assigned to the treated and control groups have ‘potential outcomes’ in both states but only one of these states is observed. Within the potential outcomes framework, a causal effect of a binary treatment is defined as the difference between these potential outcomes. Counterfactual analysis methods have to solve the problem that only one of these potential outcomes is observed for each individual.\nThe methods in DigiCAT are based on the potential outcomes framework and therefore carry certain assumptions inherited from the framework. A major one is the stable unit treatment assumption (SUVTA) which means that the potential outcomes for any unit does not depend on the treatment assignment of any other units. This means that there is no interference between units. Another important one is that there are no unmeasured/unmodelled confounders that are associated with the potential outcomes and with treatment assignment. Other assumptions associated with specific counterfactual analysis workflows are discussed with each method."
  },
  {
    "objectID": "04_cfmethod.html",
    "href": "04_cfmethod.html",
    "title": "4. DigiCAT Methods",
    "section": "",
    "text": "The main steps in propensity score matching (PSM) are to: 1) decide on which matching variables to include 2) fit a propensity model including these variables to estimate propensity scores (reflecting their propensity to experience a treatment variable) each individual in the sample 3) match treated individuals and control individuals with similar propensity scores 4) fit an outcome model using the matched data.\n\n\nIn order to successfully address confounding with PSM it is necessary to identify and include measures of all relevant confounders in the propensity model. It is also necessary to specify the correct functional form of relations between the matching variables and the treatment variable; an issue discussed in more detail later. Confounders are variables that are common causes of both treatment assignment and the outcome. For example, when aiming to estimate the impact of physical activity on mental health, a person’s gender might be a confounding factor as it might impact both physical activity levels and mental health outcomes. These confounders should be chosen based on theory and past research. There is no real way to test that all relevant confounders have been identified and measured, therefore, there is a strong reliance on subject matter knowledge when choosing matching variables. Drawing causal diagrams can help to lay out the hypothesised causal relations between treatment, confounders and outcome. Arguments have also been made for including variables in the propensity model that are related only to the outcome (not treatment) variable since this can increase power to detect the treatment effect. On the other hand, it has been shown that including variables related only to the treatment variable (not the outcome variable) can reduce power. It is critical to not include variables that might have been impacted by the outcome variable as this can induce seriously distort the treatment effects. For this reason, researchers often limit the selection of matching variables to those that occurred (or were measured) prior to the treatment. In practice, it may be difficult to identify and to have measures available of all possible relevant confounders, therefore, researchers tend to select a large number of potential confounders.\n\n\n\nPropensity score models can and have been estimated using a wide variety of approaches. This could include (regularised) regression approaches, machine learning tree-based methods, such as CART, and ensemble tree-based methods such as random forest or gradient boosted machines. Other machine learning techniques such as support vector machines and neural networks are also possible choices. There are pros and cons to different methods and DigiCAT provides an implementation of a selection of complementary methods. Logistic regression is provided for its high interpretability and quick computation. Two tree-based machine learning methods: random forest and gradient boosted machines will also be implemented soon because they have good flexibility for approximating complex relations between matching variables and treatments.\nLogistic regression has historically been by far the most popular method of estimating propensity scores. It and probit regression are method that can estimate the associations between a set of predictors and a binary (0 vs 1) outcome variable. This makes them suitable models for estimating propensity scores for binary treatments.\nIn a simple logistic regression with only one predictor, the probability that Y=1, which we could denote P(y_i), is predicted from a matching variables, which we could denote X_1, using the formula:\n\\[\nP(y_i) = \\frac{1}{1+e^{-(b_0 + b_1X_1)}}\n\\]\n\\(e\\) refers to the exponential function and \\(b_0+b_1 X_{1}\\) forms a linear combination with a constant \\(b_0\\) and the coefficient \\(b_1\\) which captures the effect of the predictor \\(X_1\\). This then generalises to a multiple logistic regression that can include many matching variables and their interactions.:\n\\[\nP(y_i) = \\frac{1}{1+e^{-(b_0 + b_1X_1 + b_2X_2 + ... + b_kX_k)}}\n\\]\nEach \\(b\\) coefficient captures the effect of one predictor which in the context of propensity score analysis represents the effect of each matching variable on the treatment, The model can be estimated using maximum likelihood estimation. The individual b coefficients are typically not of great interest in a propensity analysis context. Instead of key interest is the propensity score. For each individual, propensity scores are the scores predicted by the model based on the estimated b coefficients. That is the predicted \\(P(y_i)\\) scores represented the predicted probability of having received the treatment. It is these scores that are used in subsequent stages of propensity score analysis for matching or to derive weights in IPTW.\n\n\n\n\n\n\n\n\n\nNearest neighbour 1:1 matching\n\n\n\n\n\nNearest neighbour matching refers to a type of matching method that uses what is called ‘greedy matching’. In this method, a treated case is selected and the most similar control unit is matched with it. If there are multiple equally similar control units then one of them is selected at random. Then, another treated case is selected and the most similar control unit to it and so on and so forth until no more matches are possible. The simplest form of nearest neighbour matching uses matching without replacement whereby once a control unit has been matched to a treated unit, it is no longer available for matching to further treated units.\nNearest neighbour matching is also used with ‘calipers’ applied. This restricts matches only to treated and control units that are within a specified level of similarity to each other. This restriction is referred to as the ‘caliper distance’ and it helps to control the amount of ‘imbalance’ allowed between the treated and control units after matching. When calipers are applied, not all treated units will necessarily find a match; for some, there may not be any control units available that are sufficiently similar. In general, studies have found that using a caliper with nearest neighbour matching is beneficial for getting less biases treatment effect estimates (e.g., Austin, 2014). However, others have cautioned against using too strict calipers as these can have detrimental effects. Specifically, it can change the interpretation of the treatment effect from the effect of the treatment on the treated to the ‘effect of the treatment on the treated who have similar-enough controls’. The treated who have similar-enough controls might not be very representative of the underlying relevant population. In DigiCAT we currently do not impose caliper restrictions\n\n\n\n\n\n\n\n\n\nNearest neighbour K:1 matching\n\n\n\n\n\nNearest neighbour k:1 matching (also known as ‘many to one’ matching) is when each treated case is matched to multiple control cases. The ‘k’ refers to the number of controls that get matched to each treated unit. K:1 matching is done to make more use of the available sample as compared to 1:1 matching. In 1:1 matching if a treatment is rare (e.g., 50 out of a sample of 1000 experience it) then 1:1 matching leads to a lot of the sample not being used on the analysis (in this case only 100 out of the 1000 would be used at most).\nIt is important to note that matching more controls to each treated unit should not logically give a more accurate estimate of the effect of treatment. Rather, the main benefit of k:1 matching is that it can increase the precision of the estimate of the treatment effect.\nThe availability of k:1 matching raises the question of how many controls to match to each treated unit. Austin (2010) frame this decision in terms of a variance-bias trade-off. Specifically they point out that if you increase the the number of control units you can increase the matched sample size and thereby the precision (this is the ‘variance’ part of the trade-off). However, this likely means that you have to match control units that are less similar to their corresponding treated units. This could make the estimate of the treatment effect less accurate (the ‘bias’ part of the trade-off). They found using a simulation study that 1:1 nearest neighbour matching gave the most accurate treatment effects. Balancing various considerations they recommended that for most researchers, 1;1 or 2:1 matching (I.e., matching each treated unit to either 1 or 2 control units) is likely to the best option.\nDigiCAT offers k:1 nearest neighbour matching. A slider on the ‘balancing’ page allows you to select the number of controls you wish to match to each treated unit. Using k:1 matching might make sense if your treatment variable is quite rare within your sample. If you choose k:1 matching, we recommend you also try 1:1 matching and compare the results to see if your conclusions are similar.\n\n\n\n\n\n\n\n\n\nOptimal matching\n\n\n\n\n\nOptimal matching is a matching algorithm which creates matches based on the criterion of minimising the average dis-similarity within pairs of treated and control units. In this way it differs from nearest neighbour matching which does not necessarily optimise the overall within-pair differences. It may be helpful to check the balance achieved with optimal versus nearest neighbour matching and choose the approach that gives the best balance\n\n\n\n\n\n\nBalance checking refers to assessing whether after matching on the propensity score, treated and control units are sufficiently similar in their matching variable distributions. A large variety of methods of checking balance have been suggested. These include methods for looking at overall balance (i.e., a summary measure of balance across all matching variables) and methods for looking at balance in the individual covariates. As regards, individual covariates, originally this was done using statistical tests (e.g. a t-test or chi-square test); however, nowadays this is generally avoided since these tests depend on sample size. Instead, measures such as standardised mean differences (SMDs) are used to quantify bias, complemented with graphical displays. Previous studies have discussed potential SMD thresholds to decide whether balance is suitably met, with different authors proposing \\(|.05|\\), \\(|.10|\\), and \\(|.25|\\). It has also been noted that lower thresholds are needed for binary matching variables to be equivalent to continuous variables \\(|.1|\\) for binary variables is roughly equivalent to \\(|.25|\\) for continuous variables). However, which threshold a user prefers depends on what level of imbalance a user is willing to accept (also see the discussion on calipers). Irrespective, for transparency and to aid the interpretation of findings, it is good practice to present the SMDs for the covariates when writing up. Where there is some imbalance between the groups, adjusting for the matching variables in the outcome model can be helpful for addressing any bias due to this (see ‘outcome model’ section).\nBoth SMDs and graphical displays are implemented within DigiCAT and are provided after fitting the propensity model and implementing the matching. This allows you to inspect the quality of matches before proceeding to the outcome model. If the balance is poor, you may consider using a different method to try and get better balance. For example, you could try a different method of estimating propensity scores, switch from k:1 to 1:1 matching, or switch between nearest neighbour or optimal matching.\nIn terms of the output of the balancing stage DigiCAT, provides the common support graph which shows the distribution of propensity scores in the treatment and control group. When multiple imputation is used it shows the averaged distributions across all the imputed datasets. Ideally there should be a good amount of overlap between these two distributions, indicating that it is possible to identify a subset of the controls who are well-matched to the treated cases.\nAn observation table is also provided, which summarises the number of treated and control units before and after matching. It, therefore, shows the number of successful matches and the number of units that were discarded. The number of units are expressed in terms of the ‘effective sample size’ (ESS) which is relevant when there is weighting involved. This is because the number of cases might not in those cases be a good reflection of sample size.\nLove plots are used to display the standardised mean differences between the treated and control groups before and after matching. The orange dots represent the difference before matching (I.e., unadjusted) while the blue dots represent the difference after matching. Ideally the latter dots should all be close to the zero line. Further, one would expect to see a reduction in the standardised mean difference after matching so that the blue dots should be closer to the zero line than the orange dots.\nA balance table provides the standardised mean difference figures. Different researchers may be willing to accept different sizes of standardised mean differences (and this may differ across contexts) and there is no one right answer as to how big is too big. Commonly suggested thresholds for standardised mean differences include magnitudes of .10 and .25. Whichever threshold is chosen, however, it is recommended that these figures are reported (perhaps as Supplementary Materials) when writing up as they capture how successfully the treated and control groups have been balanced.\n\n\n\nThe final step in the PSM analysis workflow is fitting the outcome model. This is actually often much simpler than estimating the propensity model. It involves fitting a linear regression model to the now-matched data with the treatment indicator as a predictor. There are also good arguments for including the matching variables in this model too. This is because it can increase the power to detect the treatment effect, help deal with remaining bias due to imperfect balancing of the treated and control groups, and address dependencies in the data due to the fact that the data are now matched. However, this might not deal with all potential bias due to remaining imbalance because the treatment might also interact with the matching variables (e.g., different genders might benefit more or less from a treatment). This means that it can be good practice to also include treatment by matching variable interactions in the outcome model too. When this is done, a ‘marginal effects’ method can be used to calculate the average treatment effect. This is done because the coefficient for the treatment effect in the regression model might not otherwise have a meaningful interpretation, due to the presence of the interactions in the model.\n\n\nMarginal effects provide an estimate of the difference between potential outcomes under treated and control conditions and are therefore very helpful for understanding the effect of a treatment in counterfactual analysis. In DigiCAT, marginal effects are offered for binary treatments (in PSM and IPTW) using a method known as G-computation. In general this works in the following way: the outcome model has been fit, the predicted score for each case is calculated setting treatment status = treated and treatment status = control. This provides predicted outcome values for each case for each potential outcome (i.e., as if treated and as if not treated). The mean of these estimated potential outcomes across all cases is then taken for treatment status= treated and treatment status = control. The difference between these two means gives the estimated treatment effect. For PSM where we estimate the ATT, only the potential outcomes are estimated for the treated group as the ATT is all about estimating the effect of a treatment on the treated. This means that only the contrast between the potential outcomes of the treated (not control) cases is of interest. The standard errors of the treatment effect are also calculated."
  },
  {
    "objectID": "04_cfmethod.html#propensity-score-matching-psm",
    "href": "04_cfmethod.html#propensity-score-matching-psm",
    "title": "4. DigiCAT Methods",
    "section": "",
    "text": "The main steps in propensity score matching (PSM) are to: 1) decide on which matching variables to include 2) fit a propensity model including these variables to estimate propensity scores (reflecting their propensity to experience a treatment variable) each individual in the sample 3) match treated individuals and control individuals with similar propensity scores 4) fit an outcome model using the matched data.\n\n\nIn order to successfully address confounding with PSM it is necessary to identify and include measures of all relevant confounders in the propensity model. It is also necessary to specify the correct functional form of relations between the matching variables and the treatment variable; an issue discussed in more detail later. Confounders are variables that are common causes of both treatment assignment and the outcome. For example, when aiming to estimate the impact of physical activity on mental health, a person’s gender might be a confounding factor as it might impact both physical activity levels and mental health outcomes. These confounders should be chosen based on theory and past research. There is no real way to test that all relevant confounders have been identified and measured, therefore, there is a strong reliance on subject matter knowledge when choosing matching variables. Drawing causal diagrams can help to lay out the hypothesised causal relations between treatment, confounders and outcome. Arguments have also been made for including variables in the propensity model that are related only to the outcome (not treatment) variable since this can increase power to detect the treatment effect. On the other hand, it has been shown that including variables related only to the treatment variable (not the outcome variable) can reduce power. It is critical to not include variables that might have been impacted by the outcome variable as this can induce seriously distort the treatment effects. For this reason, researchers often limit the selection of matching variables to those that occurred (or were measured) prior to the treatment. In practice, it may be difficult to identify and to have measures available of all possible relevant confounders, therefore, researchers tend to select a large number of potential confounders.\n\n\n\nPropensity score models can and have been estimated using a wide variety of approaches. This could include (regularised) regression approaches, machine learning tree-based methods, such as CART, and ensemble tree-based methods such as random forest or gradient boosted machines. Other machine learning techniques such as support vector machines and neural networks are also possible choices. There are pros and cons to different methods and DigiCAT provides an implementation of a selection of complementary methods. Logistic regression is provided for its high interpretability and quick computation. Two tree-based machine learning methods: random forest and gradient boosted machines will also be implemented soon because they have good flexibility for approximating complex relations between matching variables and treatments.\nLogistic regression has historically been by far the most popular method of estimating propensity scores. It and probit regression are method that can estimate the associations between a set of predictors and a binary (0 vs 1) outcome variable. This makes them suitable models for estimating propensity scores for binary treatments.\nIn a simple logistic regression with only one predictor, the probability that Y=1, which we could denote P(y_i), is predicted from a matching variables, which we could denote X_1, using the formula:\n\\[\nP(y_i) = \\frac{1}{1+e^{-(b_0 + b_1X_1)}}\n\\]\n\\(e\\) refers to the exponential function and \\(b_0+b_1 X_{1}\\) forms a linear combination with a constant \\(b_0\\) and the coefficient \\(b_1\\) which captures the effect of the predictor \\(X_1\\). This then generalises to a multiple logistic regression that can include many matching variables and their interactions.:\n\\[\nP(y_i) = \\frac{1}{1+e^{-(b_0 + b_1X_1 + b_2X_2 + ... + b_kX_k)}}\n\\]\nEach \\(b\\) coefficient captures the effect of one predictor which in the context of propensity score analysis represents the effect of each matching variable on the treatment, The model can be estimated using maximum likelihood estimation. The individual b coefficients are typically not of great interest in a propensity analysis context. Instead of key interest is the propensity score. For each individual, propensity scores are the scores predicted by the model based on the estimated b coefficients. That is the predicted \\(P(y_i)\\) scores represented the predicted probability of having received the treatment. It is these scores that are used in subsequent stages of propensity score analysis for matching or to derive weights in IPTW.\n\n\n\n\n\n\n\n\n\nNearest neighbour 1:1 matching\n\n\n\n\n\nNearest neighbour matching refers to a type of matching method that uses what is called ‘greedy matching’. In this method, a treated case is selected and the most similar control unit is matched with it. If there are multiple equally similar control units then one of them is selected at random. Then, another treated case is selected and the most similar control unit to it and so on and so forth until no more matches are possible. The simplest form of nearest neighbour matching uses matching without replacement whereby once a control unit has been matched to a treated unit, it is no longer available for matching to further treated units.\nNearest neighbour matching is also used with ‘calipers’ applied. This restricts matches only to treated and control units that are within a specified level of similarity to each other. This restriction is referred to as the ‘caliper distance’ and it helps to control the amount of ‘imbalance’ allowed between the treated and control units after matching. When calipers are applied, not all treated units will necessarily find a match; for some, there may not be any control units available that are sufficiently similar. In general, studies have found that using a caliper with nearest neighbour matching is beneficial for getting less biases treatment effect estimates (e.g., Austin, 2014). However, others have cautioned against using too strict calipers as these can have detrimental effects. Specifically, it can change the interpretation of the treatment effect from the effect of the treatment on the treated to the ‘effect of the treatment on the treated who have similar-enough controls’. The treated who have similar-enough controls might not be very representative of the underlying relevant population. In DigiCAT we currently do not impose caliper restrictions\n\n\n\n\n\n\n\n\n\nNearest neighbour K:1 matching\n\n\n\n\n\nNearest neighbour k:1 matching (also known as ‘many to one’ matching) is when each treated case is matched to multiple control cases. The ‘k’ refers to the number of controls that get matched to each treated unit. K:1 matching is done to make more use of the available sample as compared to 1:1 matching. In 1:1 matching if a treatment is rare (e.g., 50 out of a sample of 1000 experience it) then 1:1 matching leads to a lot of the sample not being used on the analysis (in this case only 100 out of the 1000 would be used at most).\nIt is important to note that matching more controls to each treated unit should not logically give a more accurate estimate of the effect of treatment. Rather, the main benefit of k:1 matching is that it can increase the precision of the estimate of the treatment effect.\nThe availability of k:1 matching raises the question of how many controls to match to each treated unit. Austin (2010) frame this decision in terms of a variance-bias trade-off. Specifically they point out that if you increase the the number of control units you can increase the matched sample size and thereby the precision (this is the ‘variance’ part of the trade-off). However, this likely means that you have to match control units that are less similar to their corresponding treated units. This could make the estimate of the treatment effect less accurate (the ‘bias’ part of the trade-off). They found using a simulation study that 1:1 nearest neighbour matching gave the most accurate treatment effects. Balancing various considerations they recommended that for most researchers, 1;1 or 2:1 matching (I.e., matching each treated unit to either 1 or 2 control units) is likely to the best option.\nDigiCAT offers k:1 nearest neighbour matching. A slider on the ‘balancing’ page allows you to select the number of controls you wish to match to each treated unit. Using k:1 matching might make sense if your treatment variable is quite rare within your sample. If you choose k:1 matching, we recommend you also try 1:1 matching and compare the results to see if your conclusions are similar.\n\n\n\n\n\n\n\n\n\nOptimal matching\n\n\n\n\n\nOptimal matching is a matching algorithm which creates matches based on the criterion of minimising the average dis-similarity within pairs of treated and control units. In this way it differs from nearest neighbour matching which does not necessarily optimise the overall within-pair differences. It may be helpful to check the balance achieved with optimal versus nearest neighbour matching and choose the approach that gives the best balance\n\n\n\n\n\n\nBalance checking refers to assessing whether after matching on the propensity score, treated and control units are sufficiently similar in their matching variable distributions. A large variety of methods of checking balance have been suggested. These include methods for looking at overall balance (i.e., a summary measure of balance across all matching variables) and methods for looking at balance in the individual covariates. As regards, individual covariates, originally this was done using statistical tests (e.g. a t-test or chi-square test); however, nowadays this is generally avoided since these tests depend on sample size. Instead, measures such as standardised mean differences (SMDs) are used to quantify bias, complemented with graphical displays. Previous studies have discussed potential SMD thresholds to decide whether balance is suitably met, with different authors proposing \\(|.05|\\), \\(|.10|\\), and \\(|.25|\\). It has also been noted that lower thresholds are needed for binary matching variables to be equivalent to continuous variables \\(|.1|\\) for binary variables is roughly equivalent to \\(|.25|\\) for continuous variables). However, which threshold a user prefers depends on what level of imbalance a user is willing to accept (also see the discussion on calipers). Irrespective, for transparency and to aid the interpretation of findings, it is good practice to present the SMDs for the covariates when writing up. Where there is some imbalance between the groups, adjusting for the matching variables in the outcome model can be helpful for addressing any bias due to this (see ‘outcome model’ section).\nBoth SMDs and graphical displays are implemented within DigiCAT and are provided after fitting the propensity model and implementing the matching. This allows you to inspect the quality of matches before proceeding to the outcome model. If the balance is poor, you may consider using a different method to try and get better balance. For example, you could try a different method of estimating propensity scores, switch from k:1 to 1:1 matching, or switch between nearest neighbour or optimal matching.\nIn terms of the output of the balancing stage DigiCAT, provides the common support graph which shows the distribution of propensity scores in the treatment and control group. When multiple imputation is used it shows the averaged distributions across all the imputed datasets. Ideally there should be a good amount of overlap between these two distributions, indicating that it is possible to identify a subset of the controls who are well-matched to the treated cases.\nAn observation table is also provided, which summarises the number of treated and control units before and after matching. It, therefore, shows the number of successful matches and the number of units that were discarded. The number of units are expressed in terms of the ‘effective sample size’ (ESS) which is relevant when there is weighting involved. This is because the number of cases might not in those cases be a good reflection of sample size.\nLove plots are used to display the standardised mean differences between the treated and control groups before and after matching. The orange dots represent the difference before matching (I.e., unadjusted) while the blue dots represent the difference after matching. Ideally the latter dots should all be close to the zero line. Further, one would expect to see a reduction in the standardised mean difference after matching so that the blue dots should be closer to the zero line than the orange dots.\nA balance table provides the standardised mean difference figures. Different researchers may be willing to accept different sizes of standardised mean differences (and this may differ across contexts) and there is no one right answer as to how big is too big. Commonly suggested thresholds for standardised mean differences include magnitudes of .10 and .25. Whichever threshold is chosen, however, it is recommended that these figures are reported (perhaps as Supplementary Materials) when writing up as they capture how successfully the treated and control groups have been balanced.\n\n\n\nThe final step in the PSM analysis workflow is fitting the outcome model. This is actually often much simpler than estimating the propensity model. It involves fitting a linear regression model to the now-matched data with the treatment indicator as a predictor. There are also good arguments for including the matching variables in this model too. This is because it can increase the power to detect the treatment effect, help deal with remaining bias due to imperfect balancing of the treated and control groups, and address dependencies in the data due to the fact that the data are now matched. However, this might not deal with all potential bias due to remaining imbalance because the treatment might also interact with the matching variables (e.g., different genders might benefit more or less from a treatment). This means that it can be good practice to also include treatment by matching variable interactions in the outcome model too. When this is done, a ‘marginal effects’ method can be used to calculate the average treatment effect. This is done because the coefficient for the treatment effect in the regression model might not otherwise have a meaningful interpretation, due to the presence of the interactions in the model.\n\n\nMarginal effects provide an estimate of the difference between potential outcomes under treated and control conditions and are therefore very helpful for understanding the effect of a treatment in counterfactual analysis. In DigiCAT, marginal effects are offered for binary treatments (in PSM and IPTW) using a method known as G-computation. In general this works in the following way: the outcome model has been fit, the predicted score for each case is calculated setting treatment status = treated and treatment status = control. This provides predicted outcome values for each case for each potential outcome (i.e., as if treated and as if not treated). The mean of these estimated potential outcomes across all cases is then taken for treatment status= treated and treatment status = control. The difference between these two means gives the estimated treatment effect. For PSM where we estimate the ATT, only the potential outcomes are estimated for the treated group as the ATT is all about estimating the effect of a treatment on the treated. This means that only the contrast between the potential outcomes of the treated (not control) cases is of interest. The standard errors of the treatment effect are also calculated."
  },
  {
    "objectID": "04_cfmethod.html#iptw",
    "href": "04_cfmethod.html#iptw",
    "title": "4. DigiCAT Methods",
    "section": "IPTW",
    "text": "IPTW\nInverse propensity of treatment weighting (IPTW) is a counterfactual analysis approach that attempts to balance treated and control units through the use of weights. The first step is identical to propensity matching and involves fitting a propensity model to get propensity scores (see propensity model estimation above). In this model, the treatment variable is predicted by the matching variables in a model such as a logistic regression. Those scores are then transformed to provide weights. The balance of the weighted groups is then checked. Finally, the weights are used in a weighted regression. These steps are discussed in more detail below.\n\nEstimating weights for ATE\nAfter a propensity model has been fit (as described earlier) the scores from that model are taken and transformed to weights for use in IPTW. Different weights can be derived to estimate the ATT versus the ATE but here we focus on the ATE which is what is currently implemented within DigiCAT. When estimating the ATE, weights are assigned to each case such that treated cases with a lower propensity of experiencing the treatment are up-weighted and control cases with a higher propensity of experiencing the treatment are up-weighted relative to other treated and control cases respectively. This is done by transforming the estimated propensity scores according to the formula:\n\\[\nw_{i} = Z_{i}/ e_{i} + (1-Z_{i}) /(1-e_{i})\n\\]\nwhere \\(Z_{i}\\) indicates whether a case was treated or not and \\(e_{i}\\) indicates the inverse of the propensity of receiving the treatment (based on the estimated propensity score). This means that a cases weight is the inverse of the probability of receiving the treatment they actually received. This weighting scheme helps to re-balance the treated and control groups, making them more similar to each other.\n\n\nBalance Checking\nAfter estimating the IPTW weights balance checking can be used to see if the weights successfully rebalanced the treated and control groups. This can be done by looking at the differences between the groups in their matching variables after they have been weighted through looking at group (treated versus control) mean differences and graphical displays. Though it is not a measure of balance it is also possible to see at this stage how the weighting affected the ‘effective sample size’ of the data. When there is not a lot of overlap in the propensity scores of the treated and control groups, the effective sample size could be reduced quite a lot relative to the raw sample size in the unweighted data because most cases will not be very informative for a balanced comparison.\nIn DigiCAT, at the balancing check stay the common support graph is shown for the propensity scores. This indicates the similarity of the treated and control groups with greater overlap in the distributions being desirable. For IPTW it also shows the initial and weighted effective sample sizes to provide information about the effect of weighting. Typically we would expect to see that the effective sample size is smaller than the original sample size after weighting. A love plot is also shown to provide a visualisation of the group differences before and after weighting. A reference line a zero or ‘no difference’ between the treated and control group is included. Ideally the blue dots representing the standardised mean differences between the treated and control group for each matching variable should follow this zero line closely. They should also be closer to the zero line than the orange dots representing the standardised mean differences between the treated and control groups prior to matching. The values for the standardised mean differences before and after weighting are provided in the ‘Balance Table’ tab. As with propensity score matching there is no one agreed-upon magic threshold under which these values should be; however, the smaller the better and no more than .25 and ideally &lt; .05 or .10 are good figures to have in mind when judging balance.\n\n\nOutcome model fitting\nThe final stage in IPTW is the outcome model fitting. This is achieved by incorporating the IPTW weights into a weighted regression model. The options are otherwise identical to those discussed under ‘outcome model fitting’ section for PSM."
  },
  {
    "objectID": "04_cfmethod.html#non-bipartite-nbp-methods",
    "href": "04_cfmethod.html#non-bipartite-nbp-methods",
    "title": "4. DigiCAT Methods",
    "section": "Non-bipartite (NBP) methods",
    "text": "Non-bipartite (NBP) methods\nThe majority of matching methods available, including the other methods described in DigiCAT, are ‘bipartite’, which is fitting for designs with only two treatment options (one treatment group and one control group). However, in practice you may encounter a scenario in which participants may receive multiple different treatments or have different dosages of a treatment. For example, participants may adhere to one of several treatments to stop smoking or drinking – standard care, self-help and counselling-guided intervention, interactive computer programs, or a combination of these. Another scenario may be if we are investigating the number of hours of social media consumption on anxiety, for example, or the number of hours of sleep on wellbeing – in which case, the treatment is on a continuous scale, rather than dichotomous. In order to determine causal inference in such situations, nonbipartite matching methods have been suggested in place of bipartite methods.\nA common example where NBP is helpful is when the treatment variable is ordinal. This may be relevant if participants receive different dosages of a treatment and the question is ‘do cases that receive a higher dose have better outcomes than those who receive a lower dose?’ In this case if we imagine that there are three treatment groups: 0= control, 1= moderate dose, and 2= high dose and we can assume that the distance between 0 vs 1 and 1 vs 2 is equal (I.e., we have an ordinal treatment variable) we can match cases with treatment status 0 vs 1 and 1 vs 2 on their propensity score. This allows us to estimate the effect of moving from one treatment dosage to the next highest dosage.\nWe do this first by estimating propensity scores using an ordinal logistic regression model (which is suitable for ordinal outcomes) and then using those scores to match 0 vs 1 and 1 vs 2 cases. The matching is done using an optimal matching method (see ‘toptimal matching’). However, the particular implementation of this algorithm for NBP disallows matches between cases showing too much dissimilarity in their propensity scores. This is to enforce better balance between the treated and control groups.\nAfter the matching of higher and lower dosage cases has taken place we then fit an outcome model using linear regression, similar to those in PSM and IPTW to the matched data. In this way the NBP workflow is very similar to the PSM workflow in DigiCAT, however, some differences should be highlighted that are a function of the fact that methods for ordinal treatments are less developed than methods for binary treatments. First, complex survey design variables are not yet implemented for these methods. Second, in the balancing output in the love plot, the matched sample standardised mean difference is displayed with the average difference across all pairwise matches. This makes it not completely analogous plots in the PSM and IPTW output. Finally, as marginal effects are not yet implemented for NBP, only two outcome model options are available: a linear regression with only the treatment variable as a predictor and a regression with the treatment variable and matching variables as covariates. This is because in the presence of treatment-matching variable interactions the regression coefficient for the treatment effect is not likely to be meaningful on its own (I.e., without computing marginal effects)."
  },
  {
    "objectID": "05_missing.html",
    "href": "05_missing.html",
    "title": "5. Missing Data",
    "section": "",
    "text": "It’s likely that your data will be subject to non-response, and therefore contain missing values. Units (participants, patients) may be selected into your sample but refuse to take part in your survey, they may shy from answering certain questions, and there may be defective measures, deletion of outliers, or a ‘missing by design’ approach.\nHowever, to handle missingness assumptions must be made regarding the process or cause of nonresponse, in addition to model assumptions. The simplest solution is of course to limit the occurrence of missingness. However, where this is not possible, there are some options which DigiCAT employs to help you navigate this situation to facilitate your counterfactual analysis."
  },
  {
    "objectID": "05_missing.html#intro-to-missing-data",
    "href": "05_missing.html#intro-to-missing-data",
    "title": "5. Missing Data",
    "section": "",
    "text": "It’s likely that your data will be subject to non-response, and therefore contain missing values. Units (participants, patients) may be selected into your sample but refuse to take part in your survey, they may shy from answering certain questions, and there may be defective measures, deletion of outliers, or a ‘missing by design’ approach.\nHowever, to handle missingness assumptions must be made regarding the process or cause of nonresponse, in addition to model assumptions. The simplest solution is of course to limit the occurrence of missingness. However, where this is not possible, there are some options which DigiCAT employs to help you navigate this situation to facilitate your counterfactual analysis."
  },
  {
    "objectID": "05_missing.html#unititem-non-response",
    "href": "05_missing.html#unititem-non-response",
    "title": "5. Missing Data",
    "section": "Unit/item non-response",
    "text": "Unit/item non-response\nYou may hear of a distinction between ‘unit’ and ‘item’ nonresponse. Unit nonresponse would refer to a situation in which entire units are not observed – they may refuse to take part in your survey; item nonresponse would refer to a sampled unit that responds to some but not all questions or measures. Although there is ambiguity – unit nonresponse may be considered an extreme version of item nonresponse, and as such can be treated via the same techniques. However, in practice, chosen techniques may be tailored to treat item or unit nonresponse."
  },
  {
    "objectID": "05_missing.html#patterns",
    "href": "05_missing.html#patterns",
    "title": "5. Missing Data",
    "section": "Patterns",
    "text": "Patterns\nYou should consider the pattern of missingness in your data when justifying your theory and application of techniques to account for missing values. Below we briefly describe common missing data patterns.\nUnivariate missing pattern: If you have two variables, and one is completely observed whilst the other contains missing values, this would be illustrative of a univariate missing data pattern. This would remain the same if we had more than two variables – if we had five variables, and only one was not completely observed, we would have a univariate missing data pattern.\nMonotone missing pattern: Unlike univariate missing data, monotone missing data describes a situation in which we have more than one variable with missing values. A monotone missing data pattern can be demonstrated if, once a missing value occurs in one row of a variable, all subsequent values in that row are missing too. This is commonly found in longitudinal, or panel, data. A univariate missing pattern is often considered a special case of a monotone missing pattern.\nBlock/file-matching missing pattern: this pattern is frequently obtained via multiple matrix sampling/split questionnaire designs. In this pattern, any observed data point can be reached via another observed data point through a series of vertical/horizontal moves.\nArbitrary missing pattern: Perhaps the most common pattern, an arbitrary pattern can be illustrated if we consider two variables (X1, X2), and for some units X1 is observed but X2 is missing, and for other units X2 is observed but X1 is missing."
  },
  {
    "objectID": "05_missing.html#missing-data-mechanisms",
    "href": "05_missing.html#missing-data-mechanisms",
    "title": "5. Missing Data",
    "section": "Missing data mechanisms",
    "text": "Missing data mechanisms\nThe missing data mechanism (MDM) is the process generating the binary response variable that tells us whether values of the variables within our dataset are observed or not.\nMissing Complete at Random (MCAR): If the probability of missingness is equal for all cases, data are ‘missing completely at random’, and this suggests that the cause of unobserved values is not associated with the data – whether our values are observed or not does not depend on our data.\n\nExample scenario: Say we have two variables, X1 and X2, and we assume that X1 is not completely observed (there are missings) but X2 is (there are no missings). The X1 variable would be considered MCAR if the probability of observing or not observing X1 is the same for all possible values of variable X2 and all observed X1 variables.\n\nMissing at Random (MAR): If the probability of missingness is equal for all cases within groups defined by observed data, the data are said to be ‘missing at random’. That is, if the probability of missingness depends on the observed, not unobserved, data.\n\nExample scenario: Let’s take our earlier variables, X1 and X2, and assume they are negatively correlated. For illustration, X1 may represent annual income, whilst X2 may represent family size. If those units with a smaller family size are more likely to not complete the annual income question, then missing annual income (X1) values would be considered MAR.\n\nMissing Not at Random (MNAR): The most complex missing mechanism to consider, missing not at random describes the situation in which the probability of missingness varies according to an unknown.\n\nExample scenario: Demonstrated once again by our X1 and X2 variables, if individuals with a high X1 value (e.g., high annual income) tend to not complete the X1 (annual income) question, even within households with the same family size (X2), then missing values would be MNAR.\n\nAssumptions about these missing data mechanisms are critical in deciding how to handle your missing data problem. You should note that the MDM may vary over variables and units within your dataset. For instance, one variable may be assumed MCAR, whilst another may be MNAR.\nMost simple or quick fixes assume, often unrealistically, that data are MCAR. If this assumption is breached, analyses will provide biased inferences. There exist some techniques to probe the MDM and assumptions, such as Little’s MCAR test (only applicable to quantitative data), inspection of missingness patterns, and descriptive information. For example, proportions of missingness per variable may help to crudely examine the amount of missing information – the higher the amount of missingness, the heavier the weight ascribed to the correctness of MDM assumptions and thus the method used to handle this missingness. Assessment of missing data patterns may aid us in deciding which handling technique to select and visualisations may inform us whether the MDM is selective or not. However, unfortunately there is little available to help decipher if data are MAR or MNAR, and this assumption rests on empirical results and theory."
  },
  {
    "objectID": "05_missing.html#approaches-available-in-digicat",
    "href": "05_missing.html#approaches-available-in-digicat",
    "title": "5. Missing Data",
    "section": "Approaches available in DigiCAT",
    "text": "Approaches available in DigiCAT\n\nComplete case analysis\nIf your dataset contains missing values, standard software that is intended to perform complete-data analyses may either stop processing and present an error message, or analyse only the completely observed cases; they are seldom prepared to handle datasets with missingness. Therefore, as in the case of the latter example, the software may perform pragmatic decisions, without theoretical justification, in order to allow your analysis to proceed. However, in cases such as this, your inferences may be invalid. Perhaps the most common ad-hoc method, as described here, is complete case analysis (CCA), in which only the completely observed cases are entered into the analysis. CCA will yield valid inferences if the MDM is MCAR, as the observed values would therefore be considered a random sample from the complete dataset. Yet, as we known from our knowledge on assumptions, generally CCA may lead to invalid inferences if missing values are MAR or MNAR.\n\n\nNon-response weighting\nMany datasets include non-response weights or ‘attrition’ weights that can be used to deal within missing data under a ‘missing at random’ (MAR) assumption (see above for the definition of MAR). These weights account for bias due to unit- or case-wise non-response by up-weighting cases that were unlikely to be observed in the sample and down-weighting those that were highly likely to be observed in the sample. This helps to make the sample more similar to the underlying ‘complete’ sample of people who are and aren’t missing. It does so by incorporating non-response weights in a weighted regression in the same way as sampling weights are incorporated within complex sampling designs (see ‘complex survey data’).\n\n\nMultiple imputation\nWhen we refer to imputation, we refer to the process in which we replace missing values in our dataset with plausible values that may have been observed. Unlike the methods discussed above, imputation allows us to make use of all of our data, and to use standard methods, designed for complete-data analysis. For example, if we had a variable, ‘height’, which contained a missing value, we may replace it with a single plausible value – such as the mean of the non-missing values for this variable. This would be an example of a single imputation method. However, in practice our variables likely associate, and we must further acknowledge that there is some degree of uncertainty about our estimate (we are not 100% sure our imputed value is the true value). Single imputation methods do not account for these factors.\nHowever, an approach that does recognise this uncertainty is ‘multiple imputation’ (MI). MI acknowledges this uncertainty by creating multiple versions of imputed datasets, with each dataset representing one possible complete dataset, with different estimates of the missing values. Generally, these estimates are the result of a regression model, in which incomplete variables are the outcome, and complete variables are predictors. An iterative algorithm employs Bayesian estimation to update these regression parameters, to ensure that imputations are not drawn from one single set, and that new estimates generate each set of imputations.\nWe can choose how many datasets (often noted ‘m’) to create; the default in DigiCAT is m = 5. Here, we would create 5 datasets, and each dataset would be analysed separately. This would result in 5 regression models, for example. In order to get our final set of results in the form of one single inference, we would combine (pool) the results (e.g., estimates, standard errors) of our 5 models using what is known as Rubin’s rules. This gives us our model estimates by averaging the separate coefficients, and allows us to observe the uncertainty within and across imputations, via confidence intervals.\n\n\n\n\n\n\nCaveats\n\n\n\n\n\nMI is often denoted as a gold-standard method of handling missingness. However, if the imputations are poorly constructed, the validity of the results will be questioned. MI, although elegant in its simplicity and now commonly used, remains a complex method. As such, it can be difficult to spot nuances of a poor imputation model. However, there are some quick ways to check if your imputations have gone wrong. These techniques include checking the convergence of your imputation models, usually in the form of spaghetti plots, and examining the distributional discrepancy (either graphically or via descriptive statistics), which is the difference in the distribution of imputed values against the distribution of observed values. Although the nature of the MAR mechanism would elicit some differences between observed and imputed data, anything overly dramatic unaccounted for by the data features would indicate misspecification of the imputation model. These diagnostics can be obtained from DigiCAT, in addition to a guide on how to interpret them, below.\n\n\n\n\n\n\n\n\n\nPractical Considerations\n\n\n\n\n\nUseful questions to ask:\n\nIn my dataset, what percentage of units have data available for all variables in the analysis model?\nIf I used another method, such as complete case analysis, how much data would be discarded?\nWhat patterns does my missing data follow? Are they regular?\nWould my analysis benefit from MI? Is it appropriate?\n\nIf, after consulting your data, you believe MI would be an appropriate next step, you must then consider developing the imputation model that will generate your imputations. A common question may be “what variables should I include?”:\n\nGuidance within the MI literature would implore you to include at least all the variables from the intended analysis in the imputation model, in order to preserve the relationship between variables of interest.\nIf possible, it is recommended that you also include variables that are not in the analysis model (termed auxiliary variables). For example, if you have access to repeated measurements of variables in your analysis model, these could be appropriate auxiliary variables, as they would correlate with incomplete variables, and aid the imputation model in predicting the missing values. These can be included in DigiCAT by selecting them as ‘additional covariates’.\n\nResearch also would advocate for inclusion, if possible, of predictors of missingness in the imputation model to improve MAR assumption plausibility and reduce the demand to adjust for MNAR mechanisms.\n\nBy default, DigiCAT will, by default, aim to include all the data you upload in the imputation model. The algorithm employed ( MICE – multivariate imputation by chained equations) will provide a ‘predictor matrix’, and the default setting is that each variable predicts all others. If, however, the algorithm detects multicollinearity in your data, it will automatically aim to resolve this issue via removal of one (or more) predictors. If this occurs, the event will be ‘logged’, and you may request both the logged events and the predictor matrix of your imputations from DigiCAT.\nSo, you may now have an idea of what variables to include in your imputation analysis. Your next question may be “what model should I use?”\nThe method leveraged in DigiCAT uses random forest as an ‘imputation model’ to guess what the missing values would have been had they been observed. Random forest is a type of machine learning method that produces and aggregates the results of lots of decision trees and has the advantage of tending to perform better than traditional regression methods for prediction. In multiple imputation models it has the advantage of being able to approximate non-linear functions so it reduces the dependence on having to correctly specify the imputation model. In general, these defaults are well-informed and may cover your data well. However, in some cases, another method may be superior. For further reading, you may consult the provided references to learn more about other methods which may better suit the needs of your data.\n“How many imputed datasets do I need?”\nIn DigiCAT, the number of imputed datasets (m) is set to 5 by default, in order to accept a wide range of datasets, without being too computationally demanding. However, you may wish to alter m, based on the amount of missings. There is no definitive number for m, however, many guidelines and rules of thumb have been suggested, which we cover below. If you wish to increase m beyond this limit, we implore you to download the R script at the end of the analysis and re-run it with more imputed datasets on your own machine.\nWhy would you want to adjust m? Essentially, as imputed estimates are derived from a random sample of m datasets, the estimates include random sampling variation (imputation variation). This means that estimates from m datasets are variable (thus may be considered inefficient and non-replicable), in comparison to asymptotic estimates generated if the data were imputed an infinite number of times. These issues can be reduced by increasing m.\nHistorically, as few as 3-10 imputations have been considered sufficient. However, this may be specific to point estimates only; if we also want to obtain replicable standard error (SE) estimates, this often may need to be increased, and some suggestions stretch as far as m = 200."
  },
  {
    "objectID": "06_survey.html",
    "href": "06_survey.html",
    "title": "6. Complex Survey Data",
    "section": "",
    "text": "Complex survey data refers to data that have been collected according to an explicit sampling scheme that deviates from a simple random sample (where everyone in a target population has the same chance of being selected into a sample). This might be done to save data collection resources, or to make sure that there are sufficient numbers of a small group in the sample.\nDue to their advantages, complex sampling designs are very common in large-scale survey data. These datasets usually provide variables relating to their complex sampling designs so that users can take them into account in their analyses.\nDatasets might include stratification, clustering, and/or sampling weights (sometimes called ‘survey’ or ‘design weights’) which correspond to the different characteristics of complex sampling. Stratification is when the population is divided into relatively homogeneous groups and a number of pre-determined units is sampled from each. Stratification variables indicate to which stratum a case belongs. For example, a population might be divided into areas with different levels of deprivation and units sampled within each of those to help ensure that different levels of deprivation are well represented. Clustering is when units are sampled in ‘clumps’, for example, rather than randomly sampling households within a whole large area, people within a subset of neighbourhoods within that area might be sampled. Clustering variables thus indicate to which cluster a case belongs. Sampling weights reflect the unequal chances of people being selected into a sample. Cases that had a high probability of selection will have low weights and people who had a low probability of selection will have high weights.This means that when it comes to the analysis, under-represented groups (relative to the population) will be correspondingly up-weighted and over-represented groups will be down-weighted. Note that this is quite analogous to how weights work in IPTW, except that in that case the weighting is to do with making two groups more similar to each other rather than making a sample more similar to an underlying population.\nIf a dataset has been collected using a complex survey sampling design it is important to take this into account in analyses of those datasets if the research questions relate to the underlying population that the sample was taken from. For example, ignoring clustering will tend to give standard errors that are too small and will inflate statistical significance. Ignoring sampling weights can result in the wrong values for the treatment estimates.\nDigiCAT currently implements a ‘design-based’ approach to dealing with complex survey data. This means that it uses information about how the data were sampled (I.e., any sampling, stratification, and clustering variables) when doing the analysis. It specifically uses a ‘pseudomaximum likelihood estimation’ (PML) approach with a Taylor Series Linearisation. PML is a form of weighted estimation that replaces the usual sample statistics usually used with a weighted version. Taylor Series Linearisation provides a correction to the standard errors."
  },
  {
    "objectID": "06_survey.html#what-is-complex-survey-data",
    "href": "06_survey.html#what-is-complex-survey-data",
    "title": "6. Complex Survey Data",
    "section": "",
    "text": "Complex survey data refers to data that have been collected according to an explicit sampling scheme that deviates from a simple random sample (where everyone in a target population has the same chance of being selected into a sample). This might be done to save data collection resources, or to make sure that there are sufficient numbers of a small group in the sample.\nDue to their advantages, complex sampling designs are very common in large-scale survey data. These datasets usually provide variables relating to their complex sampling designs so that users can take them into account in their analyses.\nDatasets might include stratification, clustering, and/or sampling weights (sometimes called ‘survey’ or ‘design weights’) which correspond to the different characteristics of complex sampling. Stratification is when the population is divided into relatively homogeneous groups and a number of pre-determined units is sampled from each. Stratification variables indicate to which stratum a case belongs. For example, a population might be divided into areas with different levels of deprivation and units sampled within each of those to help ensure that different levels of deprivation are well represented. Clustering is when units are sampled in ‘clumps’, for example, rather than randomly sampling households within a whole large area, people within a subset of neighbourhoods within that area might be sampled. Clustering variables thus indicate to which cluster a case belongs. Sampling weights reflect the unequal chances of people being selected into a sample. Cases that had a high probability of selection will have low weights and people who had a low probability of selection will have high weights.This means that when it comes to the analysis, under-represented groups (relative to the population) will be correspondingly up-weighted and over-represented groups will be down-weighted. Note that this is quite analogous to how weights work in IPTW, except that in that case the weighting is to do with making two groups more similar to each other rather than making a sample more similar to an underlying population.\nIf a dataset has been collected using a complex survey sampling design it is important to take this into account in analyses of those datasets if the research questions relate to the underlying population that the sample was taken from. For example, ignoring clustering will tend to give standard errors that are too small and will inflate statistical significance. Ignoring sampling weights can result in the wrong values for the treatment estimates.\nDigiCAT currently implements a ‘design-based’ approach to dealing with complex survey data. This means that it uses information about how the data were sampled (I.e., any sampling, stratification, and clustering variables) when doing the analysis. It specifically uses a ‘pseudomaximum likelihood estimation’ (PML) approach with a Taylor Series Linearisation. PML is a form of weighted estimation that replaces the usual sample statistics usually used with a weighted version. Taylor Series Linearisation provides a correction to the standard errors."
  },
  {
    "objectID": "06_survey.html#complex-survey-data-in-psm",
    "href": "06_survey.html#complex-survey-data-in-psm",
    "title": "6. Complex Survey Data",
    "section": "Complex survey data in PSM",
    "text": "Complex survey data in PSM\nWhen estimating the population ATT (PATT) it is important to take the survey design into account in the outcome model. This means that any stratification, clustering, and weighting variables must feature in the linear regression outcome model that comes at the end of the PSM workflow. It does so using the PML and Taylor Series Linearisation solution mentioned above.\nPrevious research has been less clear on whether complex survey design variables need to be taken into account at earlier stages of the PSM workflow, namely when estimating the propensity model and when assessing balance. Some authors have made conceptual arguments against fitting design-adjusted propensity models and instead including survey weights as a covariate (DuGoff et al., 2014). This is based on the idea that propensity scores are inherently concerns with samples rather than populations and so there is no need to generalise them to a population. When researchers have used simulation studies to explore this issue they have found inconsistent evidence and this has led some to propose that it doesn’t really make an important difference (Ridgeway et al. 2015; Lenis et al., 2019; Austin et al., 2018). However, one quite comprehensive simulation study did find that for continuous outcomes like the ones that can be studied with DigiCAT there were benefits to fitting a design-adjusted model for propensity score estimation and for taking complex survey variables into account when assessing balance (Austin et al., 2018). This was further supported by another comprehensive simulation study (Lenis et al., 2019 ). For this reason, in DigiCAT if users supply complex survey variables, these are also taken into account when estimating the propensity score with a logistic regression (using a weighted regression) and when assessing matching variable balance (using weighted standardised mean differences). The way that they are taken into account in the logistic regression is very similar to the way they feature in the linear regression outcome model."
  },
  {
    "objectID": "06_survey.html#missing-data-and-complex-survey-data",
    "href": "06_survey.html#missing-data-and-complex-survey-data",
    "title": "6. Complex Survey Data",
    "section": "Missing data and complex survey data",
    "text": "Missing data and complex survey data\nIt is very common in practice for datasets to have both missing data AND complex survey variables. This can be quite a complicated situation to deal with; however, because it is such a frequent feature of real data we provide options for dealing with this in DigiCAT. First, we allow for non-response weights to be used. Non-response weights are weights that take into account the fact that some cases are not likely to be missing totally (or completely) at random. Instead, maybe people who have the highest levels of mental health issues dropped out of a sample. Provided that the risk of non-response can be estimated from available data, non-response weights can be produced and used to up-weight those who are less likely to have provided outcome data and down-weight those who are more likely to. Have. This helps deal with bias due to non-response. When there are sampling weights and non-response dataset providers often produce a combined ‘sampling + non-response’ weight that takes into account both the unequal probabilities of sampling and the non-random non-response. Details of these variables are typically provided with dataset documentation. DigiCAT allows users to supply these weights to be used in an analysis (perhaps alongside stratification and cluster variables) in the same way that sampling weights (see above) are used.\nDigiCAT also offers the option of multiple imputation with complex survey design variables. Here the workflow is similar to the usual multiple imputation workflow but the design variables are included as features in the imputation model and adjusted for in the outcome model."
  },
  {
    "objectID": "07_further.html",
    "href": "07_further.html",
    "title": "Acknowledgements & Further reading",
    "section": "",
    "text": "The following features are currently in development or planned for incorporation soon:\n\nMachine learning propensity score estimation methods (generalised boosting models and random forest)\nCounterfactual analysis for continuous and nominal treatments\nCounterfactual analysis for binary outcomes\nMarginal effects workflow for ordinal treatment variables\nMore options for multiple imputation\nMore options for IPTW (including ATT estimation and weight stabilisation)"
  },
  {
    "objectID": "07_further.html#digicat-features-in-development",
    "href": "07_further.html#digicat-features-in-development",
    "title": "Acknowledgements & Further reading",
    "section": "",
    "text": "The following features are currently in development or planned for incorporation soon:\n\nMachine learning propensity score estimation methods (generalised boosting models and random forest)\nCounterfactual analysis for continuous and nominal treatments\nCounterfactual analysis for binary outcomes\nMarginal effects workflow for ordinal treatment variables\nMore options for multiple imputation\nMore options for IPTW (including ATT estimation and weight stabilisation)"
  },
  {
    "objectID": "07_further.html#acknowledgements",
    "href": "07_further.html#acknowledgements",
    "title": "Acknowledgements & Further reading",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe are grateful to the Wellcome Trust for funding the Discovery and Prototyping phases of the development of DigiCAT. Our thanks also go to the many who contributed to DigiCAT in various ways. We are grateful to the young person advisory groups (YPAGs) who provided invaluable insights and help guide the direction of DigiCAT, to the users who responded to our user survey and to the group members who provided feedback on iterations of the tool."
  },
  {
    "objectID": "07_further.html#further-reading",
    "href": "07_further.html#further-reading",
    "title": "Acknowledgements & Further reading",
    "section": "Further Reading",
    "text": "Further Reading\n\nAustin, Peter C., Nathaniel Jembere, and Maria Chiu. 2016. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57. https://doi.org/10.1177/0962280216658920.\nAustin, P. C., & Stuart, E. A. (2015). Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies. Statistics in Medicine, 34(28), 3661–3679.\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\nde Vries, B. B. P., & Groenwold, R. H. (2017). A comparison of two approaches to implementing propensity score methods following multiple imputation. Epidemiology, Biostatistics, and Public Health, 14(4).\nDesai, R. J., & Franklin, J. M. (2019). Alternative approaches for confounding adjustment in observational studies using weighting based on the propensity score: A primer for practitioners. Bmj, 367.\nLenis, David, Trang Quynh Nguyen, Nianbo Dong, and Elizabeth A. Stuart. 2019. “It’s All about Balance: Propensity Score Matching in the Context of Complex Survey Data.” Biostatistics 20 (1): 147–63. https://doi.org/10.1093/biostatistics/kxx063.\nLu, B., Greevy, R., Xu, X., & Beck, C. (2011). Optimal nonbipartite matching and its statistical applications. The American Statistician, 65(1), 21–30."
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterpretation interpretation interpretation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n Solution \n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n Optional hello my optional friend\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DigiCAT Tutorials",
    "section": "",
    "text": "Welcome to the tutorial pages for DigiCAT! The tutorials provide information on what counterfactual analysis is, why it can be useful, and how to do it using DigiCAT. These pages are designed to be accessible to those with minimal experience of counterfactual analysis. As such, where we provide technical details, we have also endeavoured to provide a translation; a ‘what does this mean?’. If after reading through the tutorials you crave more information on counterfactual analysis you can check out the further reading."
  },
  {
    "objectID": "index.html#overview-of-tutorials",
    "href": "index.html#overview-of-tutorials",
    "title": "DigiCAT Tutorials",
    "section": "",
    "text": "Welcome to the tutorial pages for DigiCAT! The tutorials provide information on what counterfactual analysis is, why it can be useful, and how to do it using DigiCAT. These pages are designed to be accessible to those with minimal experience of counterfactual analysis. As such, where we provide technical details, we have also endeavoured to provide a translation; a ‘what does this mean?’. If after reading through the tutorials you crave more information on counterfactual analysis you can check out the further reading."
  },
  {
    "objectID": "index.html#how-to-engage-with-tutorials",
    "href": "index.html#how-to-engage-with-tutorials",
    "title": "DigiCAT Tutorials",
    "section": "How to engage with tutorials",
    "text": "How to engage with tutorials\nThe tutorials are structured such that users can read only a sub-section of the sections relevant for their analysis and tailored to their level of background knowledge.\nIf you are a first-time user of counterfactual analysis we recommend reading the section on Introduction to counterfactual analysis. If you need some help choosing a counterfactual analysis approach, we recommend reading choosing a counterfactual analysis approach. If you are already confident in your knowledge of counterfactual analysis and know which approach you would like to implement you can go directly to the relevant tutorial sections for information on how it is implemented in DigiCAT. There are specific sections for currently implemented methods, which are: propensity score analysis (with 1:1 and k:1 matching), propensity score weighting, and non-bipartite propensity score analysis. All of these method can be used with survey design variables and with either multiple imputation, weighting, or complete case analysis to deal with missing data. These also have their own sections in the tutorials."
  }
]