[
  {
    "objectID": "01_howto_digicat.html",
    "href": "01_howto_digicat.html",
    "title": "1. The DigiCAT Tool",
    "section": "",
    "text": "Counterfactual analysis provides a set of tools that can help us understand what the active ingredients are in mental health. While the gold standard for doing so would be something like a randomised controlled trial (RCT), RCTs are not always possible and when they are, they are expensive and challenging to do well. Their highly controlled nature also means that they may lack ecological validity so that results from RCTs might not apply very well in the real world.\nAs such there is a lot of value in being able to get an idea of what might impact mental health from observational data (I.e., non-experimental data). In fact, there is a wealth of pre-existing mental health data ready to be used in this way. Some examples of such datasets can be found in, for example, this catalogue of mental health datasets: https://www.cataloguementalhealth.ac.uk/\nHowever, we noticed that counterfactual analysis is not as widely used in mental health research as one might expect. There may be a number of reasons for this but we think a major one is that counterfactual analysis might not be very accessible for mental health researchers. It would not typically be part of training and it can be technically quite complex, which may limit researcher’s ability to use it. There’s also a lack of accessible tools for counterfactual analysis. Most tools require coding skills and while some web applications exist they don’t do everything. In particular, their functionality tends to be narrowly focused on a particular type of analysis and they tend not be able to deal with the ‘real world’ features of data like the presence of missingness or complex survey data or non-binary treatments. These are things we wanted to address with DigiCAT.\nAnother reason we made DigiCAT is we noticed that it took rather a long time to code counterfactual analyses using existing packages and things got complicated when we had to consider things like missingness, clustering, large numbers of confounders etc. By pulling together all the code from common ‘journeys’ and creating an easy ‘point and click’ interface we also aim to speed up the implementation of counterfactual analysis even for expert users adept in the technique in existing softwares like R."
  },
  {
    "objectID": "01_howto_digicat.html#why-develop-digicat",
    "href": "01_howto_digicat.html#why-develop-digicat",
    "title": "1. The DigiCAT Tool",
    "section": "",
    "text": "Counterfactual analysis provides a set of tools that can help us understand what the active ingredients are in mental health. While the gold standard for doing so would be something like a randomised controlled trial (RCT), RCTs are not always possible and when they are, they are expensive and challenging to do well. Their highly controlled nature also means that they may lack ecological validity so that results from RCTs might not apply very well in the real world.\nAs such there is a lot of value in being able to get an idea of what might impact mental health from observational data (I.e., non-experimental data). In fact, there is a wealth of pre-existing mental health data ready to be used in this way. Some examples of such datasets can be found in, for example, this catalogue of mental health datasets: https://www.cataloguementalhealth.ac.uk/\nHowever, we noticed that counterfactual analysis is not as widely used in mental health research as one might expect. There may be a number of reasons for this but we think a major one is that counterfactual analysis might not be very accessible for mental health researchers. It would not typically be part of training and it can be technically quite complex, which may limit researcher’s ability to use it. There’s also a lack of accessible tools for counterfactual analysis. Most tools require coding skills and while some web applications exist they don’t do everything. In particular, their functionality tends to be narrowly focused on a particular type of analysis and they tend not be able to deal with the ‘real world’ features of data like the presence of missingness or complex survey data or non-binary treatments. These are things we wanted to address with DigiCAT.\nAnother reason we made DigiCAT is we noticed that it took rather a long time to code counterfactual analyses using existing packages and things got complicated when we had to consider things like missingness, clustering, large numbers of confounders etc. By pulling together all the code from common ‘journeys’ and creating an easy ‘point and click’ interface we also aim to speed up the implementation of counterfactual analysis even for expert users adept in the technique in existing softwares like R."
  },
  {
    "objectID": "01_howto_digicat.html#how-to-use-digicat",
    "href": "01_howto_digicat.html#how-to-use-digicat",
    "title": "1. The DigiCAT Tool",
    "section": "How to use DigiCAT",
    "text": "How to use DigiCAT\nThere are two ways to use DigiCAT. The first is to download it and use it on your local computer. This is recommended if you need to avoid uploading your data to a server or if you have a very large dataset or very computationally intensive analyses. The second way to use it is via the web application. The two versions are mostly very similar but the web application has a few more limits on it.\nR Package\nTo use DigiCAT on your local computer you should have a recent version of R installed. Within R you can then enter the following lines to install and then run DigiCAT. Once it is installed you only need to run the third line to launch it each time.\n\ninstall.packages(\"remotes\") \nremotes::install_github(\"josiahpjking/DigiCAT@develop\")\nDigiCAT::run_DigiCAT(enableLocal = TRUE)\n\nWeb Version\nThe web version of DigiCAT is available at https://digicatapp.shinyapps.io/DigiCAT\n\nTODO Tree diagram of DigiCAT options"
  },
  {
    "objectID": "01_howto_digicat.html#involving-lived-experience-expertise-alongside-the-tool",
    "href": "01_howto_digicat.html#involving-lived-experience-expertise-alongside-the-tool",
    "title": "1. The DigiCAT Tool",
    "section": "Involving Lived Experience Expertise alongside the Tool",
    "text": "Involving Lived Experience Expertise alongside the Tool\n\nWhy should the research use Lived Experience expertise ?\n\nReminder of the subjective nature of mental health\n\nBring the Qualitative into the Quantitative analysis\n\nStrengthen your research. Increasingly, co-production and PPI (patient and public involvement) is required for funding. This should be taken seriously, rather than treated as an add-on or in a tokenistic way.\n\nPractical application – How does it relate to real experiences? Hone in on practically relevant research questions. Facilitate clinical validity of the interpretation of research findings\n\nAssess likely efficacy in real world application of findings (if applicable)\n\nPositive impact for lived experience collaborators. Empowerment, purpose, acquisition of new skills and knowledge, and having a role in changing and improving mental health research and interventions.\n\n\n\nHow should the research use Lived Experience expertise?\nThere is not one single definition of lived experience, and so it can come in many forms. It might include those who have previously or continue to experience mental illness difficulties, but it can also include family members of those who have experienced it.\nThose with lived experience may already be working in the academy, so it is also fruitful to engage with colleagues (across disciplines) who may bring in a unique lived experience perspective.\nFor example, in exploring active ingredients of mental health, a young person’s advisory group were asked to rank their importance, explain reasons, and add other things. This exercise demonstrated that having access to ‘social media’ was a more important factor than initially expected by the researchers.\nUpon further exploration in the advisory group, it was revealed that ‘social media’ was important in order to keep in touch with friends. As one young person pointed out: “It’s very important to have a lot of friends and to really stick together because they are one of the only people that are there in all of the situations”. This suggests that exploring use of social media would have been misleading without understanding why the young people thought this was important.\nSimilarly, ‘recovery’ can mean different things to different people, and understanding alternative meanings and framings can have a substantial positive impact on the research.\nAs researchers (and individuals) we come to what we do with our set of biases and expectations, it is therefore a good idea to run our interpretations by third parties with lived experience in the area we are wishing to inform.\nIn addition to publishing in scientific journals it is important to present the findings in ways that are easy to understand and utilise for those for whom they are most practically relevant; that is service users and generally those with lived experience. Such communications are best produced in collaboration with them.\nCollaboration with lived experience experts increases credibility and practical reliability and applications of research findings."
  },
  {
    "objectID": "02_introcf.html",
    "href": "02_introcf.html",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "",
    "text": "Counterfactual analysis is a set of techniques for trying to understand the impact of a treatment. It does so by comparing people who experienced that treatment to a hypothetical ‘counterfactual’ version of themselves who did not. Since in the real world, people either do or not experience a treatment, we cannot literally compare the same person under the scenario of experiencing versus not experiencing a treatment. Instead controls (who did not experience the treatment) who are very similar to the treated people in all important respects are used to stand in for the counterfactual. By ‘very similar in all important respects’ we mean that although they did not experience the treatment, they had a very similar propensity to or ‘similar chances of’ experiencing the treatment’ as a person who did experience the treatment. This propensity is estimated based on their levels of ‘confounding factors’ I.e., factors that impact both the likelihood of receiving the treatment and the outcome of interest. We can compare treated and control units on an outcome of interest (e.g., depression) and if those outcomes differ we can be more sure that it is due to the treatment, rather than any confounding factors. As such, counterfactual analysis can support causal inference in a way that models that don’t take into account confounding factors cannot. As we discuss in these tutorials, there are several methods by which propensities can be estimated and used to support causal inference in observational data.\nAs an example of where counterfactual analysis can be useful, we might ask whether social media has a negative impact on adolescents’ mental health. We might find an association between social media use and depression in adolescents. However, we can’t be sure yet that this association is not due to confounding. For example, maybe adolescents with poorer mental health tend to turn to social media more (and are also more likely to continue to have poorer mental health ,given the stability of mental health over time). Or maybe there are other confounding factors, like attention deficit hyperactivity disorder (ADHD) that might lead people to use more social media and separately increase their risk of depression. If the set of potential confounding factors in this association can be identified and measured they could be used in a counterfactual analysis to see if there is still an association between social media use and depression after accounting for confounding. People with similar propensities to use social media a lot but who differ in their actual use could be compared. If those who use social media more show higher depression levels, we can be more sure that this is actually due to the social media use, rather than other confounding factors. We will use the example of social media use and mental health as an example throughout these tutorials as social media use was identified by our young person advisory group as a factor that they thought it particularly important to explore in research on young people’s mental health."
  },
  {
    "objectID": "02_introcf.html#what-is-counterfactual-analysis",
    "href": "02_introcf.html#what-is-counterfactual-analysis",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "",
    "text": "Counterfactual analysis is a set of techniques for trying to understand the impact of a treatment. It does so by comparing people who experienced that treatment to a hypothetical ‘counterfactual’ version of themselves who did not. Since in the real world, people either do or not experience a treatment, we cannot literally compare the same person under the scenario of experiencing versus not experiencing a treatment. Instead controls (who did not experience the treatment) who are very similar to the treated people in all important respects are used to stand in for the counterfactual. By ‘very similar in all important respects’ we mean that although they did not experience the treatment, they had a very similar propensity to or ‘similar chances of’ experiencing the treatment’ as a person who did experience the treatment. This propensity is estimated based on their levels of ‘confounding factors’ I.e., factors that impact both the likelihood of receiving the treatment and the outcome of interest. We can compare treated and control units on an outcome of interest (e.g., depression) and if those outcomes differ we can be more sure that it is due to the treatment, rather than any confounding factors. As such, counterfactual analysis can support causal inference in a way that models that don’t take into account confounding factors cannot. As we discuss in these tutorials, there are several methods by which propensities can be estimated and used to support causal inference in observational data.\nAs an example of where counterfactual analysis can be useful, we might ask whether social media has a negative impact on adolescents’ mental health. We might find an association between social media use and depression in adolescents. However, we can’t be sure yet that this association is not due to confounding. For example, maybe adolescents with poorer mental health tend to turn to social media more (and are also more likely to continue to have poorer mental health ,given the stability of mental health over time). Or maybe there are other confounding factors, like attention deficit hyperactivity disorder (ADHD) that might lead people to use more social media and separately increase their risk of depression. If the set of potential confounding factors in this association can be identified and measured they could be used in a counterfactual analysis to see if there is still an association between social media use and depression after accounting for confounding. People with similar propensities to use social media a lot but who differ in their actual use could be compared. If those who use social media more show higher depression levels, we can be more sure that this is actually due to the social media use, rather than other confounding factors. We will use the example of social media use and mental health as an example throughout these tutorials as social media use was identified by our young person advisory group as a factor that they thought it particularly important to explore in research on young people’s mental health."
  },
  {
    "objectID": "02_introcf.html#what-kind-of-research-questions-can-be-answered-with-counterfactual-analysis",
    "href": "02_introcf.html#what-kind-of-research-questions-can-be-answered-with-counterfactual-analysis",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "What kind of research questions can be answered with counterfactual analysis?",
    "text": "What kind of research questions can be answered with counterfactual analysis?\nCounterfactual analysis can be used to address research questions concerned with the causal impact of some candidate treatment or exposure on some outcome variable of interest. Usually this is in observational data where it is not possible to control whether people experience a treatment or not. It is particularly valuable when it is suspected that people do not randomly experience a treatment or not.\nIt needs three main things:\n\nA treatment variable (e.g., social media use)\n\nAn outcome variable (e.g., depression)\n\nConfounders or ’matching variables (e.g., ADHD, prior depression, sex/gender, socioeconomic status, physical health etc.)\n\nOther than the social media use example above, examples of research questions that could be asked using counterfactual analysis include:\n\nWhat is the impact of reading for pleasure on mental health?\nDoes school exclusion lead to poorer labour market outcomes?\nDo more positive relationships with teachers lead to fewer behavioural problems in adolescence?\n\nWhen thinking about the different research questions that can be answered, it is useful to make a distinction between two key treatment effects. The first is the ‘ATT’ which stands for the average effect of treatment on the treated. The other is the ‘ATE’ which is the ‘average treatment effect’ and captures the effect of the treatment over the entire population. That is, it estimates what the effect would be at the population level if entire population moved from untreated to treated. Some methods can give you ATE while others can give you ATT so it is important to think about which it is you are interested in when deciding on your approach (also see ‘Choosing a counterfactual analysis approach’). The key question to ask are ‘is the treatment likely to make sense for/be feasible for the whole population or just a sub-section of the population?’ If the answer is ‘no’ this suggests that the ATT may be or more interest than the ATE. In some cases, both ATT and ATE may be of interest."
  },
  {
    "objectID": "02_introcf.html#what-counterfactual-analysis-approaches-are-offered-in-this-tool",
    "href": "02_introcf.html#what-counterfactual-analysis-approaches-are-offered-in-this-tool",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "What counterfactual analysis approaches are offered in this tool?",
    "text": "What counterfactual analysis approaches are offered in this tool?\nDigiCAT currently offers several broad approaches to counterfactual analysis: propensity score matching for binary (traditional propensity score analysis) and ordinal treatment variables (non-bipartite optimal matching) and propensity score weighting for binary variables (inverse propensity of treatment weighting; IPTW). Propensity score matching for binary treatments is based on matching treated and control units with similar propensity scores to achieve balance between treated and control groups. Propensity weighting for binary treatments also uses a propensity score but transforms it into a weight that is used in a weighted regression. The weighted regression up-weights some cases and down-weights others to achieve balance between the treated and control groups. Non-bipartite propensity scoring estimates a propensity score based on treating the treatment variable as ordinal and then matches cases that have similar propensity scores but different treatment levels.\nWe are continuing to develop DigiCAT to make more approaches available. Please see X for a list of features that are currently under development. Feel free to get in touch uoe_digicat-group@uoe.onmicrosoft.com if you have suggestions for approaches you would like to see within the tool."
  },
  {
    "objectID": "02_introcf.html#why-not-just-adjust-for-covariates-in-regression",
    "href": "02_introcf.html#why-not-just-adjust-for-covariates-in-regression",
    "title": "2. Introduction to Counterfactual Analysis",
    "section": "Why not just adjust for covariates in regression?",
    "text": "Why not just adjust for covariates in regression?\nPerhaps the most widely used approach to attempting to deal with confounding is to adjust for potential confounding within a regression or ANOVA-type model. This approach chould be described as ‘conditional adjustment’. However, counterfactual analysis provides a more principled method that places causal inference at the fore. Importantly, it includes steps to check and ensure good covariate balance has been achieved. This is not guaranteed and may fail in traditional covariate adjustment approaches, leading to biases when trying to understand the causal effect of a treatment without any warning signs for the user. Further, counterfactual analysis provides a way of more clearly defining the target population about which inferences are being made. It can also readily incorporate a very large number of covariates. None of this comes at the cost of Interpretability when using methods like propensity matching or weighting. In DigiCAT in particular, both methods use a linear regression for the outcome model so the interpretation of the treatment effect is no more complicated than in a traditional regression model."
  },
  {
    "objectID": "03_choosecf.html",
    "href": "03_choosecf.html",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "",
    "text": "DigiCAT offers counterfactual analysis for different types of treatment variables. Binary treatment variables (where people either experience a treatment or not) can be analysed using propensity matching or weighting (IPTW). Ordinal treatment variables (where people might experience a treatment to different degrees) can be analysed using optimal non-bipartite propensity matching. DigiCAT will try to detect what type of treatment variable you have and if it has only two categories it will assume it is binary. If it has 3-5 categories it will assume it is ordinal."
  },
  {
    "objectID": "03_choosecf.html#ordinal-vs-binary-treatments",
    "href": "03_choosecf.html#ordinal-vs-binary-treatments",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "",
    "text": "DigiCAT offers counterfactual analysis for different types of treatment variables. Binary treatment variables (where people either experience a treatment or not) can be analysed using propensity matching or weighting (IPTW). Ordinal treatment variables (where people might experience a treatment to different degrees) can be analysed using optimal non-bipartite propensity matching. DigiCAT will try to detect what type of treatment variable you have and if it has only two categories it will assume it is binary. If it has 3-5 categories it will assume it is ordinal."
  },
  {
    "objectID": "03_choosecf.html#matching-vs-weighting",
    "href": "03_choosecf.html#matching-vs-weighting",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "Matching vs Weighting",
    "text": "Matching vs Weighting\nMatching and weighting approaches are both available for binary variables. Matching can be used to estimate the ATT and weighting can be used to estimate the ATT or ATE. As such, your research question (and whether it concerns the effect of a treatment in the treated or whole population) should be your primary guide your choice of matching versus weighting. There are; however, also some other differences between matching and weighting that may be considered. For example, weighting uses the whole sample, whereas matching (depending on which method is used) might result in some unmatched cases being unused. Weighting can, therefore, have advantages when the treatment is rare or the overall sample size is small. More details on these methods are provided below.\nNote that though DigiCAT implements weighting and matching, reflecting the most common ways in which propensity scores are used, these are not the only way they can be used in counterfactual analysis. For example, stratifcation on the propensity score is also sometimes used and sometimes (though we do not necessarily recommend it), the propensity score is used as a covariate in a regression as form of counterfactual analysis. DigiCAT will this also offer the option to save out estimated propensity scores so that they can be used in other ways. This also allows the scores to be used in more complex analyses that are not offered in DigiCAT, for example, using them in a structural equation model. We are grateful to the member of our user group who suggested adding this feature."
  },
  {
    "objectID": "03_choosecf.html#machine-learning-versus-regression-propensity-estimation",
    "href": "03_choosecf.html#machine-learning-versus-regression-propensity-estimation",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "Machine learning versus regression propensity estimation",
    "text": "Machine learning versus regression propensity estimation\nTODO"
  },
  {
    "objectID": "03_choosecf.html#complete-case-versus-multiple-imputation-versus-weighting-for-missing-data",
    "href": "03_choosecf.html#complete-case-versus-multiple-imputation-versus-weighting-for-missing-data",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "Complete case versus multiple imputation versus weighting for missing data",
    "text": "Complete case versus multiple imputation versus weighting for missing data\nThere are several options for dealing with missing data available in DigiCAT: complete case analysis, multiple imputation, and weighting. Complete case analysis should be selected when you have no missing data. It is also justifiable to use it if you are confident that there is no relation between missingness and the variables you have in your model or between the missingness and the missing values of the outcome variable. Weighting is a good option for dealing with case-level missing data, I.e., some people are completely missing at a time-point. This could happen if you have longitudinal data with your matching, treatment, and outcome variables measured at sequential time-points. If you have an attrition weight for the last time-point you could use it with missingness weighting to deal with any non-random drop-out between your baseline and outcome point. Finally, multiple imputation can be used to deal with case and variable-level missingness. However, it can take a long time to run so users with large datasets and large numbers of matching variables and/or additional covariates should be aware of this. More details on the missingness methods are provided below."
  },
  {
    "objectID": "03_choosecf.html#the-benefits-of-sensitivity-analyses",
    "href": "03_choosecf.html#the-benefits-of-sensitivity-analyses",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "The benefits of sensitivity analyses",
    "text": "The benefits of sensitivity analyses\nThere are a lot of subjective decision points in counterfactual analysis, such as which matching variables to include, which additional covariates to include (if any), which missingness approach to use for missing data and whether to match or weight by a propensity score. While there have been a lot of advancements in recent years in our understanding of what best practice for implementing counterfactual analysis looks like, it is not always the case that there is one ‘best approach’. Sometimes it depends. Sometimes it’s still debated or there isn’t a lot of evidence to guide a decision. As such, we recommend where relevant using multiple different approaches and checking whether you get similar results with different approaches. If you do, this helps increase confidence in your findings. If your results differ a lot across approaches then the nature of the differences might be illuminating. For example, if you find that PSM is non-significant but IPTW is significant this may be because the latter uses more of the data (e.g., if the treatment and control groups are very different). Provided IPTW successfully balances the groups, it could suggest that IPTW is more suitable for these data."
  },
  {
    "objectID": "03_choosecf.html#propensity-score-matching",
    "href": "03_choosecf.html#propensity-score-matching",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "Propensity score matching",
    "text": "Propensity score matching\n\nPropensity score specification and estimation\n\n\nNearest neighbour 1:1 matching\n\n\nNearest neighbour K:1 matching"
  },
  {
    "objectID": "03_choosecf.html#iptw",
    "href": "03_choosecf.html#iptw",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "IPTW",
    "text": "IPTW"
  },
  {
    "objectID": "03_choosecf.html#nbp",
    "href": "03_choosecf.html#nbp",
    "title": "3. Choosing a Counterfactual Analysis Approach",
    "section": "NBP",
    "text": "NBP"
  },
  {
    "objectID": "04_cfmethod.html",
    "href": "04_cfmethod.html",
    "title": "4. Counterfactual Methods",
    "section": "",
    "text": "Counterfactual analysis is based on what is known as the ‘potential outcomes’ framework which was formalised by Rubin. It is based on the notion that for each unit exposed to a treatment W (Wi = 1) there is a potential outcome (Wi = 0) where they weren’t exposed to that treatment. It is assumed that each unit assigned to the treated and control groups have ‘potential outcomes’ in both states but only one of these states is observed. Within the potential outcomes framework, a causal effect of a binary treatment is defined as the difference between these potential outcomes. Counterfactual analysis methods have to solve the problem that only one of these potential outcomes is observed for each individual.\nThe methods in DigiCAT are based on the potential outcomes framework and therefore carry certain assumptions inherited from the framework. A major one is the stable unit treatment assumption (SUVTA) which means that the potential outcomes for any unit does not depend on the treatment assignment of any other units. This means that there is no interference between units. Another important one is that there are no unmeasured/unmodelled confounders that are associated with the potential outcomes and with treatment assignment. Other assumptions associated with specific counterfactual analysis workflows are discussed with each method."
  },
  {
    "objectID": "04_cfmethod.html#the-potential-outcomes-framework",
    "href": "04_cfmethod.html#the-potential-outcomes-framework",
    "title": "4. Counterfactual Methods",
    "section": "",
    "text": "Counterfactual analysis is based on what is known as the ‘potential outcomes’ framework which was formalised by Rubin. It is based on the notion that for each unit exposed to a treatment W (Wi = 1) there is a potential outcome (Wi = 0) where they weren’t exposed to that treatment. It is assumed that each unit assigned to the treated and control groups have ‘potential outcomes’ in both states but only one of these states is observed. Within the potential outcomes framework, a causal effect of a binary treatment is defined as the difference between these potential outcomes. Counterfactual analysis methods have to solve the problem that only one of these potential outcomes is observed for each individual.\nThe methods in DigiCAT are based on the potential outcomes framework and therefore carry certain assumptions inherited from the framework. A major one is the stable unit treatment assumption (SUVTA) which means that the potential outcomes for any unit does not depend on the treatment assignment of any other units. This means that there is no interference between units. Another important one is that there are no unmeasured/unmodelled confounders that are associated with the potential outcomes and with treatment assignment. Other assumptions associated with specific counterfactual analysis workflows are discussed with each method."
  },
  {
    "objectID": "04_cfmethod.html#propensity-score-matching",
    "href": "04_cfmethod.html#propensity-score-matching",
    "title": "4. Counterfactual Methods",
    "section": "Propensity score matching",
    "text": "Propensity score matching\n\nDescription of PSM\n\n\nSteps in PSM\nThe main steps in PSM are to: 1) decide on which matching variables to include 2) fit a propensity model including these variables to estimate propensity scores (reflecting their propensity to experience a treatment variable) each individual in the sample 3) match treated individuals and control individuals with similar propensity scores 4) fit an outcome model using the matched data.\n\n\nSelecting Matching Variables\nIn order to successfully address confounding with PSM it is necessary to identify and include measures of all relevant confounders in the propensity model. It is also necessary to specify the correct functional form of relations between the matching variables and the treatment variable; an issue discussed in more detail later. Confounders are variables that are common causes of both treatment assignment and the outcome. For example, when aiming to estimate the impact of physical activity on mental health, a person’s gender might be a confounding factor as it might impact both physical activity levels and mental health outcomes. These confounders should be chosen based on theory and past research. There is no real way to test that all relevant confounders have been identified and measured, therefore, there is a strong reliance on subject matter knowledge when choosing matching variables. Drawing causal diagrams can help to lay out the hypothesised causal relations between treatment, confounders and outcome. Arguments have also been made for including variables in the propensity model that are related only to the outcome (not treatment) variable since this can increase power to detect the treatment effect. On the other hand, it has been shown that including variables related only to the treatment variable (not the outcome variable) can reduce power. It is critical to not include variables that might have been impacted by the outcome variable as this can induce seriously distort the treatment effects. For this reason, researchers often limit the selection of matching variables to those that occurred (or were measured) prior to the treatment. In practice, it may be difficult to identify and to have measures available of all possible relevant confounders, therefore, researchers tend to select a large number of potential confounders.\n\n\nPropensity score specification and estimation\nPropensity score models can and have been estimated using a wide variety of approaches. This could include (regularised) regression approaches, machine learning tree-based methods, such as CART, and ensemble tree-based methods such as random forest or gradient boosted machines. Other machine learning techniques such as support vector machines and neural networks are also possible choices. There are pros and cons to different methods and DigiCAT provides an implementation of a selection of complementary methods. Logistic regression is provided for its high interpretability and quick computation. Two tree-based machine learning methods: random forest and gradient boosted machines are provided because they have good flexibility for approximating complex relations between matching variables and treatments and because previous research has shown that they often achieve better prediction than logistic regression. Each approach is described in more detail below.\n\n\nLogistic and probit regression\nLogistic regression has historically been by far the most popular method of estimating propensity scores. It and probit regression are method that can estimate the associations between a set of predictors and a binary (0 vs 1) outcome variable. This makes them suitable models for estimating propensity scores for binary treatments. In a simple logistic regression with only one predictor, the probability that \\(Y=1\\), which we could denote \\(P(y_i)\\), is predicted from a matching variables, which we could denote \\(X_1\\), using the formula:\n\\[\nP(y_i) = \\frac{1}{1+e^{-(b_0 + b_1X_1)}}\n\\]\ne refers to the exponential function and \\(b_0+b_1 X_{1}\\) forms a linear combination with a constant \\(b_0\\) and the coefficient \\(b_1\\) which captures the effect of the predictor \\(X_1\\). This then generalises to a multiple logistic regression that can include many matching variables and their interactions.:\n\\[\nP(y_i) = \\frac{1}{1+e^{-(b_0 + b_1X_1 + b_2X_2 + ... + b_kX_k)}}\n\\]\nEach \\(b\\) coefficient captures the effect of one predictor which in the context of propensity score analysis represents the effect of each matching variable on the treatment, The model can be estimated using maximum likelihood estimation. The individual b coefficients are typically not of great interest in a propensity analysis context. Instead of key interest is the propensity score. For each individual, propensity scores are the scores predicted by the model based on the estimated b coefficients. That is the predicted \\(P(y_i)\\) scores represented the predicted probability of having received the treatment. It is these scores that are used in subsequent stages of propensity score analysis for matching or to derive weights in IPTW.\nEstimating propensity scores using probit regression is very similar to logistic regression…. In DigiCAT…\nTODO\n\nLogistic regression\nRandom forest\nGradient boosted machines /generalised boosted modeling\nIntro to GBM – what is it?\nWhy GBM for PS estimation\nTuning\nCalibrated probabilities\n\nThere are different possible ways to use the propensity scores estimated in the previous step to match treated and control cases. The goal is to try and pair treated and control units that are highly similar in their propensity scores. Matching methods differ in things like what type of algorithm is used (‘greedy’ or ‘optimal’ matching), whether matching happens with or without replacement (I.e., whether a control be matched to multiple cases), the matching ratio (e.g., 2 controls to 1 treated unit, 3:1), the order in which treated and control units are matched, and whether calipers are applied, meaning that matches are only made if treated and control cases fall within a defined threshold of ‘similarity’. Taken together, there are a very large number of options for how to match treated and control cases!\nIn DigiCAT we offer a set of methods that have generally been shown to perform well in terms of important outcomes such as giving the least biased estimates of treatment effects. These are described in turn.\n\n\nNearest neighbour 1:1 matching\nNearest neighbour matching refers to a type of matching method that uses what is called ‘greedy matching’. In this method, a treated case is selected and the most similar control unit is matched with it. If there are multiple equally similar control units then one of them is selected at random. Then, another treated case is selected and the most similar control unit to it and so on and so forth until no more matches are possible. The simplest form of nearest neighbour matching uses matching without replacement whereby once a control unit has been matched to a treated unit, it is no longer available for matching to further treated units.\nNearest neighbour matching is also used with ‘calipers’ applied. This restricts matches only to treated and control units that are within a specified level of similarity to each other. This restriction is referred to as the ‘caliper distance’ and it helps to control the amount of ‘imbalance’ allowed between the treated and control units after matching. When calipers are applied, not all treated units will necessarily find a match; for some, there may not be any control units available that are sufficiently similar. In general, studies have found that using a caliper with nearest neighbour matching is beneficial for getting less biases treatment effect estimates (e.g., Austin, 2014). However, others have cautioned against using too strict calipers as these can have detrimental effects. Specifically, it can change the interpretation of the treatment effect from the effect of the treatment on the treated to the ‘effect of the treatment on the treated who have similar-enough controls’. The treated who have similar-enough controls might not be very representative of the underlying relevant population.\nThere has been some discussion and disagreement about the best caliper width to use. Most recommendations have been based on defining a threshold on the scale of the log-odds of the propensity score, I.e., the linear predictor of the treatment taken from the logistic regression used to estimate the propensity scores. For example, one suggestion has been to take the 0.25 standard deviations of the log-odds of the propensity score, while others have argued for 0.20 standard deviations on this scale. The reality is that the right caliper width depends to some extent on the data and the willingness of the researchers to accept some bias in the estimate of the treatment effect for the sake of having ‘enough’ matches. This is because the stricter the caliper width is, the more difficult it is to find matches that meet the matching criteria.\n[XXX brief 1-2 lines on choosing caliper distance]\nIn DigiCAT nearest neighbour matching is chosen by selecting the ‘X’ option on the ‘X’ screen. This implements nearest neighbour matching with…\n\n\nNearest neighbour K:1 matching\nNearest neighbour k:1 matching (also known as ‘many to one’ matching) is when each treated case is matched to multiple control cases. The ‘k’ refers to the number of controls that get matched to each treated unit. K:1 matching is done to make more use of the available sample as compared to 1:1 matching. In 1:1 matching if a treatment is rare (e.g., 50 out of a sample of 1000 experience it) then 1:1 matching leads to a lot of the sample not being used on the analysis (in this case only 100 out of the 1000 would be used at most).\nIt is important to note that matching more controls to each treated unit should not logically give a more accurate estimate of the effect of treatment. Rather, the main benefit of k:1 matching is that it can increase the precision of the estimate of the treatment effect.\nThe availability of k:1 matching raises the question of how many controls to match to each treated unit. Austin (2010) frame this decision in terms of a variance-bias trade-off. Specifically they point out that if you increase the the number of control units you can increase the matched sample size and thereby the precision (this is the ‘variance’ part of the trade-off). However, this likely means that you have to match control units that are less similar to their corresponding treated units. This could make the estimate of the treatment effect less accurate (the ‘bias’ part of the trade-off). They found using a simulation study that 1:1 nearest neighbour matching gave the most accurate treatment effects. Balancing various considerations they recommended that for most researchers, 1;1 or 2:1 matching (I.e., matching each treated unit to either 1 or 2 control units) is likely to the best option.\nDigiCAT offers k:1 nearest neighbour matching. A slider on the XX page allows you to select the number of controls you wish to match to each treated unit. Using k:1 matching might make sense if your treatment variable is quite rare within your sample. If you choose k:1 matching, we recommend you also try 1:1 matching and compare the results to see if your conclusions are similar.\n\n\nOptimal Matching\nOptimal matching is a matching algorithm which creates matches based on the criterion of minimising the average dis-similarity within pairs of treated and control units [not much more to say about this?]\n\n\nBalance Checking\nBalance checking refers to assessing whether after matching on the propensity score, treated and control units are sufficiently similar in their matching variable distributions. A large variety of methods of checking balance have been suggested. These include methods for looking at overall balance (I.e., a summary measure of balance across all matching variables) and methods for looking at balance in the individual covariates. As regards, individual covariates, originally this was done using statistical tests (e.g. a t-test or chi-square test); however, nowadays this is generally avoided since these tests depend on sample size. Instead, measures such as standardised mean differences (SMDs) are used to quantify bias, complemented with graphical displays. Previous studies have discussed potential SMD thresholds to decide whether balance is suitably met, with different authors proposing |.05|, |.10|, and |.25|. It has also been noted that lower thresholds are needed for binary matching variables to be equivalent to continuous variables |.1| for binary variables is roughly equivalent to |.25| for continuous variables). However, which threshold a user prefers depends on what level of imbalance a user is willing to accept (also see the discussion on calipers). Irrespective, for transparency and to aid the interpretation of findings, it is good practice to present the SMDs for the covariates when writing up. Where there is some imbalance between the groups, adjusting for the matching variables in the outcome model can be helpful for addressing any bias due to this (see ‘outcome model’ section).\nIn terms of overall balance measures… [briefly discuss but focus on what’s implemented in DigiCAT] Both SMDs and graphical displays are implemented within DigiCAT and are provided after fitting the propensity model and implementing the matching. This allows users to inspect the quality of matches before proceeding to the outcome model. If the balance is poor, you may consider using a different method to try and get better balance. For example, you could try a different method of estimating propensity scores, switch from k:1 to 1:1 matching, or switch between nearest neighbour or optimal matching.\n\n\nOutcome model\nThe final step in the PSM analysis workflow is fitting the outcome model. This is actually often much simpler than estimating the propensity model. It involves fitting a linear regression model to the now-matched data with the treatment indicator as a predictor. There are also good arguments for including the matching variables in this model too."
  },
  {
    "objectID": "04_cfmethod.html#iptw",
    "href": "04_cfmethod.html#iptw",
    "title": "4. Counterfactual Methods",
    "section": "IPTW",
    "text": "IPTW\nInverse propensity of treatment weighting (IPTW) is a counterfactual analysis approach that attempts to balance treated and control units through the use of weights. The first step is identical to propensity matching and involves fitting a propensity model to get propensity scores. In this model, the treatment variable is predicted by the matching variables in a model such as a random forest or logistic regression. Those scores are then transformed to provide weights. The balance of the weighted groups is then checked. Finally, the weights are used in a weighted regression. These steps are discussed in more detail below.\n\nSteps in IPTW\nEstimating weights for ATT\nEstimating weights for ATE\nWeight stabilisation/truncation (if needed)\nBalance checking\nWeighted outcome model\n[will follow very similar structure to PSM]"
  },
  {
    "objectID": "04_cfmethod.html#nbp",
    "href": "04_cfmethod.html#nbp",
    "title": "4. Counterfactual Methods",
    "section": "NBP",
    "text": "NBP\nThe majority of matching methods available, including those described in DigiCAT, are ‘bipartite’, which is fitting for designs with only two treatment options (one treatment group and one control group). However, in practice you may encounter a scenario in which participants may receive multiple different treatments. For example, participants may adhere to one of several treatments to stop smoking or drinking – standard care, self-help and counselling-guided intervention, interactive computer programs, or a combination of these. Another scenario may be if we are investigating the number of hours of social media consumption on anxiety, for example, or the number of hours of sleep on wellbeing – in which case, the treatment is on a continuous scale, rather than dichotomous . In order to determine causal inference in such situations, nonbipartite matching methods have been suggested in place of bipartite methods.\n\nConceptually, how does nonbipartite matching work?\nIn the distance matrix tabulated below, we can imagine 6 different groups (termed ‘nodes’, in graph theory) that may be matched with one another. Some groups, such as 3 and 4, cannot be matched with one another, as the distance between them is an ‘infinite distance’.\nOptimal matching would aim to create a matching consisting of 3 pairs, which the overall smallest total distance. Thus, matching result for the example below would be pairs of 1 and 6, 2 and 4, and 3 and 5, which would give a total overall distance of 30, the smallest achievable (10 + 10 + 10). Although, for instance, nodes 1 and 2 have an even smaller distance (of just 1), this would mean the remaining pairs to be matched would yield a greater overall distance, as node 6 would be left to match either with 4 or 5 (note it cannot be matched with 3 as it is of infinite distance), which have greater distances (100), and so the overall distance would not be the smallest possible.\nThis is demonstrative of where optimal matching is superior to greedy matching, which would match nodes by proximity, but not consider overall distance and not yield the global optimal solution. In this instance, the 6 groups cannot be clustered into two, and therefore a nonbipartite algorithm should be used in place of bipartite algorithms.\n\n\n\nNode\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n\\(\\infty\\)\n1\n\\(\\infty\\)\n2\n\\(\\infty\\)\n10\n\n\n2\n1\n\\(\\infty\\)\n2\n10\n100\n\\(\\infty\\)\n\n\n3\n\\(\\infty\\)\n2\n\\(\\infty\\)\n\\(\\infty\\)\n10\n\\(\\infty\\)\n\n\n4\n2\n10\n\\(\\infty\\)\n\\(\\infty\\)\n30\n100\n\n\n5\n\\(\\infty\\)\n100\n10\n30\n\\(\\infty\\)\n100\n\n\n6\n10\n\\(\\infty\\)\n\\(\\infty\\)\n100\n100\n\\(\\infty\\)\n\n\n\nAlthough nonbipartite matching can also be used with multiple unordered groups and time varying covariates, in DigiCAT it currently focuses on matching with multiple ordinal dose groups, specifically, and should not be interpreted as otherwise unless stated. In this scenario, the general steps would be as follows: first, generate propensity scores through an ordinal logit model, and then leverage these propensity scores to measure distances between participants. Following calculation of the distances, optimal nonbipartite matching would be performed to match participants possessing similar pre-treatment characteristics. Note, it is assumed that matching variables are the only variables that influence treatment assignment.\n\n\nPropensity model estimation\n\nOrdinal logistic regression\n\n\n\nOptimal matching\n\n\nOutcome model"
  },
  {
    "objectID": "05_missing.html",
    "href": "05_missing.html",
    "title": "5. Missing Data",
    "section": "",
    "text": "It’s likely that your data will be subject to non-response, and therefore contain missing values. Units (participants, patients) may be selected into your sample but refuse to take part in your survey, they may shy from answering certain questions, and there may be defective measures, deletion of outliers, or a ‘missing by design’ approach.\nHowever, to handle missingness assumptions must be made regarding the process or cause of nonresponse, in addition to model assumptions. The simplest solution is of course to limit the occurrence of missingness. However, where this is not possible, there are some options which DigiCAT employs to help you navigate this situation to facilitate your counterfactual analysis."
  },
  {
    "objectID": "05_missing.html#intro-to-missing-data",
    "href": "05_missing.html#intro-to-missing-data",
    "title": "5. Missing Data",
    "section": "",
    "text": "It’s likely that your data will be subject to non-response, and therefore contain missing values. Units (participants, patients) may be selected into your sample but refuse to take part in your survey, they may shy from answering certain questions, and there may be defective measures, deletion of outliers, or a ‘missing by design’ approach.\nHowever, to handle missingness assumptions must be made regarding the process or cause of nonresponse, in addition to model assumptions. The simplest solution is of course to limit the occurrence of missingness. However, where this is not possible, there are some options which DigiCAT employs to help you navigate this situation to facilitate your counterfactual analysis."
  },
  {
    "objectID": "05_missing.html#unititem-non-response",
    "href": "05_missing.html#unititem-non-response",
    "title": "5. Missing Data",
    "section": "Unit/item non-response",
    "text": "Unit/item non-response\nYou may hear of a distinction between ‘unit’ and ‘item’ nonresponse. Unit nonresponse would refer to a situation in which entire units are not observed – they may refuse to take part in your survey; item nonresponse would refer to a sampled unit that responds to some but not all questions or measures. Although there is ambiguity – unit nonresponse may be considered an extreme version of item nonresponse, and as such can be treated via the same techniques. However, in practice, chosen techniques may be tailored to treat item or unit nonresponse."
  },
  {
    "objectID": "05_missing.html#patterns",
    "href": "05_missing.html#patterns",
    "title": "5. Missing Data",
    "section": "Patterns",
    "text": "Patterns\nYou should consider the pattern of missingness in your data when justifying your theory and application of techniques to account for missing values. Below we briefly describe common missing data patterns.\nUnivariate missing pattern: If you have two variables, and one is completely observed whilst the other contains missing values, this would be illustrative of a univariate missing data pattern. This would remain the same if we had more than two variables – if we had five variables, and only one was not completely observed, we would have a univariate missing data pattern.\nMonotone missing pattern: Unlike univariate missing data, monotone missing data describes a situation in which we have more than one variable with missing values. A monotone missing data pattern can be demonstrated if, once a missing value occurs in one row of a variable, all subsequent values in that row are missing too. This is commonly found in longitudinal, or panel, data. A univariate missing pattern is often considered a special case of a monotone missing pattern.\nBlock/file-matching missing pattern: this pattern is frequently obtained via multiple matrix sampling/split questionnaire designs. In this pattern, any observed data point can be reached via another observed data point through a series of vertical/horizontal moves.\nArbitrary missing pattern: Perhaps the most common pattern, an arbitrary pattern can be illustrated if we consider two variables (X1, X2), and for some units X1 is observed but X2 is missing, and for other units X2 is observed but X1 is missing."
  },
  {
    "objectID": "05_missing.html#missing-data-mechanisms",
    "href": "05_missing.html#missing-data-mechanisms",
    "title": "5. Missing Data",
    "section": "Missing data mechanisms",
    "text": "Missing data mechanisms\nThe missing data mechanism (MDM) is the process generating the binary response variable that tells us whether values of the variables within our dataset are observed or not.\nMissing Complete at Random (MCAR): If the probability of missingness is equal for all cases, data are ‘missing completely at random’, and this suggests that the cause of unobserved values is not associated with the data – whether our values are observed or not does not depend on our data.\n\nExample scenario: Say we have two variables, X1 and X2, and we assume that X1 is not completely observed (there are missings) but X2 is (there are no missings). The X1 variable would be considered MCAR if the probability of observing or not observing X1 is the same for all possible values of variable X2 and all observed X1 variables.\n\nMissing at Random (MAR): If the probability of missingness is equal for all cases within groups defined by observed data, the data are said to be ‘missing at random’. That is, if the probability of missingness depends on the observed, not unobserved, data.\n\nExample scenario: Let’s take our earlier variables, X1 and X2, and assume they are negatively correlated. For illustration, X1 may represent annual income, whilst X2 may represent family size. If those units with a smaller family size are more likely to not complete the annual income question, then missing annual income (X1) values would be considered MAR.\n\nMissing Not at Random (MNAR): The most complex missing mechanism to consider, missing not at random describes the situation in which the probability of missingness varies according to an unknown.\n\nExample scenario: Demonstrated once again by our X1 and X2 variables, if individuals with a high X1 value (e.g., high annual income) tend to not complete the X1 (annual income) question, even within households with the same family size (X2), then missing values would be MNAR.\n\nAssumptions about these missing data mechanisms are critical in deciding how to handle your missing data problem. You should note that the MDM may vary over variables and units within your dataset. For instance, one variable may be assumed MCAR, whilst another may be MNAR.\nMost simple or quick fixes assume, often unrealistically, that data are MCAR. If this assumption is breached, analyses will provide biased inferences. There exist some techniques to probe the MDM and assumptions, such as Little’s MCAR test (only applicable to quantitative data), inspection of missingness patterns, and descriptive information. For example, proportions of missingness per variable may help to crudely examine the amount of missing information – the higher the amount of missingness, the heavier the weight ascribed to the correctness of MDM assumptions and thus the method used to handle this missingness. Assessment of missing data patterns may aid us in deciding which handling technique to select and visualisations may inform us whether the MDM is selective or not. However, unfortunately there is little available to help decipher if data are MAR or MNAR, and this assumption rests on empirical results and theory."
  },
  {
    "objectID": "05_missing.html#approaches-available-in-digicat",
    "href": "05_missing.html#approaches-available-in-digicat",
    "title": "5. Missing Data",
    "section": "Approaches available in DigiCAT",
    "text": "Approaches available in DigiCAT\n\ncomplete case analysis\nIf your dataset contains missing values, standard software that is intended to perform complete-data analyses may either stop processing and present an error message, or analyse only the completely observed cases; they are seldom prepared to handle datasets with missingness. Therefore, as in the case of the latter example, the software may perform pragmatic decisions, without theoretical justification, in order to allow your analysis to proceed. However, in cases such as this, your inferences may be invalid. Perhaps the most common ad-hoc method, as described here, is complete case analysis (CCA), in which only the completely observed cases are entered into the analysis. CCA will yield valid inferences if the MDM is MCAR, as the observed values would therefore be considered a random sample from the complete dataset. Yet, as we known from our knowledge on assumptions, generally CCA may lead to invalid inferences if missing values are MAR or MNAR.\n\n\nmultiple imputation\nWhen we refer to imputation, we refer to the process in which we replace missing values in our dataset with plausible values that may have been observed. Unlike the methods discussed above, imputation allows us to make use of all of our data, and to use standard methods, designed for complete-data analysis. For example, if we had a variable, ‘height’, which contained a missing value, we may replace it with a single plausible value – such as the mean of the non-missing values for this variable. This would be an example of a single imputation method. However, in practice our variables likely associate, and we must further acknowledge that there is some degree of uncertainty about our estimate (we are not 100% sure our imputed value is the true value). Single imputation methods do not account for these factors.\nHowever, an approach that does recognise this uncertainty is ‘multiple imputation’ (MI). MI acknowledges this uncertainty by creating multiple versions of imputed datasets, with each dataset representing one possible complete dataset, with different estimates of the missing values. Generally, these estimates are the result of a regression model, in which incomplete variables are the outcome, and complete variables are predictors. An iterative algorithm employs Bayesian estimation to update these regression parameters, to ensure that imputations are not drawn from one single set, and that new estimates generate each set of imputations.\nWe can choose how many datasets (often noted ‘m’) to create; the default in DigiCAT is m = 5. Here, we would create 5 datasets, and each dataset would be analysed separately. This would result in 5 regression models, for example. In order to get our final set of results in the form of one single inference, we would combine (pool) the results (e.g., estimates, standard errors) of our 5 models using what is known as Rubin’s rules. This gives us our model estimates by averaging the separate coefficients, and allows us to observe the uncertainty within and across imputations, via confidence intervals.\nCaveats\nMI is often denoted as a gold-standard method of handling missingness. However, if the imputations are poorly constructed, the validity of the results will be questioned. MI, although elegant in its simplicity and now commonly used, remains a complex method. As such, it can be difficult to spot nuances of a poor imputation model. However, there are some quick ways to check if your imputations have gone wrong. These techniques include checking the convergence of your imputation models, usually in the form of spaghetti plots, and examining the distributional discrepancy (either graphically or via descriptive statistics), which is the difference in the distribution of imputed values against the distribution of observed values. Although the nature of the MAR mechanism would elicit some differences between observed and imputed data, anything overly dramatic unaccounted for by the data features would indicate misspecification of the imputation model. These diagnostics can be obtained from DigiCAT, in addition to a guide on how to interpret them, below.\nPractical Considerations\nUseful questions to ask:\n\nIn my dataset, what percentage of units have data available for all variables in the analysis model?\nIf I used another method, such as complete case analysis, how much data would be discarded?\nWhat patterns does my missing data follow? Are they regular?\nWould my analysis benefit from MI? Is it appropriate?\n\nIf, after consulting your data, you believe MI would be an appropriate next step, you must then consider developing the imputation model that will generate your imputations. A common question may be “what variables should I include?”:\n\nGuidance within the MI literature would implore you to include at least all the variables from the intended analysis in the imputation model, in order to preserve the relationship between variables of interest.\nIf possible, it is recommended that you also include variables that are not in the analysis model (termed auxiliary variables). For example, if you have access to repeated measurements of variables in your analysis model, these could be appropriate auxiliary variables, as they would correlate with incomplete variables, and aid the imputation model in predicting the missing values.\nResearch also would advocate for inclusion, if possible, of predictors of missingness in the imputation model to improve MAR assumption plausibility and reduce the demand to adjust for MNAR mechanisms.\n\nBy default, DigiCAT will, by default, aim to include all the data you upload in the imputation model. The algorithm employed ( MICE – multivariate imputation by chained equations) will provide a ‘predictor matrix’, and the default setting is that each variable predicts all others. If, however, the algorithm detects multicollinearity in your data, it will automatically aim to resolve this issue via removal of one (or more) predictors. If this occurs, the event will be ‘logged’, and you may request both the logged events and the predictor matrix of your imputations from DigiCAT.\nSo, you may now have an idea of what variables to include in your imputation analysis. Your next question may be “what model should I use?”\nThe MICE algorithm leveraged in DigiCAT will require a univariate imputation method for each variable that contains missingness; the selection of this method is heavily driven by measurement level – numerical, binary, categorical. Luckily, the algorithm used will detect this measurement level and the according method will be selected by default: numeric (predictive mean matching), binary - factor with 2 levels (logistic regression), nominal/unordered categorical – factor with &gt;2 levels (polytomous logistic regression), and ordinal/ordered categorical – ordered factor with &gt;2 levels (proportional odds model). In general, these defaults are well-informed and may cover your data well. However, in some cases, another method may be superior. For further reading, you may consult the provided references to learn more about other methods which may better suit the needs of your data.\nAnd “how many imputed datasets do I need?”\nIn DigiCAT, the number of imputed datasets (m) is set to 5 by default, in order to accept a wide range of datasets, without being too computationally demanding. However, you may wish to alter m, based on the amount of missings. There is no definitive number for m, however, many guidelines and rules of thumb have been suggested, which we cover below. It is possible to increase m within DigiCAT, though we set a limit at X for practical reasons. If you wish to increase m beyond this limit, we implore you to download the R script at the end of the analysis and re-run it with more imputed datasets on your own machine.\nWhy would you want to adjust m? Essentially, as imputed estimates are derived from a random sample of m datasets, the estimates include random sampling variation (imputation variation). This means that estimates from m datasets are variable (thus may be considered inefficient and non-replicable), in comparison to asymptotic estimates generated if the data were imputed an infinite number of times. These issues can be reduced by increasing m.\nHistorically, as few as 3-10 imputations have been considered sufficient. However, this may be specific to point estimates only; if we also want to obtain replicable standard error (SE) estimates, this often may need to be increased, and some suggestions stretch as far as m = 200.\nA recent two-step approach to approximating m can be tailored to your data and may provide an efficient way of deciding m, proposed by Von Hippel, below:\n\\[\nM = 1 + \\frac{1}{2} \\left( \\frac{FMI}{CV(SE)} \\right)\n\\]\nFMI is the fraction of missing information – that is, the fraction by which the squared SE would shrink if the data were complete.\nCV() is a coefficient of variation – that is, the percentage by which you’d be willing to accept the SE change if the data were re-imputed.\nHowever, there are some caveats to note to calculate this appropriately, and users should read the recommended references and further reading if they wish to follow up on this approach.\nA third alternative approach to estimate m is termed iterative multiple imputation. Here, you would keep increasing m until estimates converge, or variation becomes negligible with the addition of further datasets.\n“How many iterations do I need?”\nIt’s important that all imputations for all variables reach convergence, and this can be visually identified via inspection of convergence plots. To reach convergence, generally 20-30 iterations are needed, or even less. The default in DigiCAT is 20. You may examine convergence via requesting spaghetti plots in DigiCAT.\n[which links to… diagnosis and evaluation of imputations]\n\nReading convergence plots\nExamining imputed values\nDistributional discrepancy/comparisons between observed versus imputed data\nRegression diagnostics - residuals\nCross-validation\nPPC"
  },
  {
    "objectID": "06_survey.html",
    "href": "06_survey.html",
    "title": "6. Complex Survey Data",
    "section": "",
    "text": "Complex survey data refers to data that have been collected according to an explicit sampling scheme that deviates from a simple random sample (where everyone in a target population has the same chance of being selected into a sample). This might be done to save data collection resources, or to make sure that there are sufficient numbers of a small group in the sample.\nDue to their advantages, complex sampling designs are very common in large-scale survey data. These datasets usually provide variables relating to their complex sampling designs so that users can take them into account in their analyses.\nDatasets might include stratification, clustering, and/or sampling weights (sometimes called ‘survey’ or ‘design weights’) which correspond to the different characteristics of complex sampling. Stratification is when the population is divided into relatively homogeneous groups and a number of pre-determined units is sampled from each. Stratification variables indicate to which stratum a case belongs. For example…..[XXX]. Clustering is when units are sampled in ‘clumps’, for example, rather than randomly sampling households within a whole large area, people within a subset of neighbourhoods within that area might be sampled. Clustering variables thus indicate to which cluster a case belongs. Sampling weights reflect the unequal chances of people being selected into a sample. Cases that had a high probability of selection will have low weights and people who had a low probability of selection will have have high weights. This means that when it comes to the analysis, under-represented groups (relative to the population) will be correspondingly up-weighted and over-represented groups will be down-weighted.\nIf a dataset has been collected using a complex survey sampling design it is important to take this into account in analyses of those datasets if the research questions relate to the underlying population that the sample was taken from. For example, ignoring clustering will tend to give standard errors that are too small and will inflate statistical significance. Ignoring sampling weights can result in the wrong values for the treatment estimates.\nDigiCAT currently implements a ‘design-based’ approach to dealing with complex survey data. This means that it uses information about how the data were sampled (I.e., any sampling, stratification, and clustering variables) when doing the analysis. It specifically uses a ‘pseudomaximum likelihood estimation’ (PML) approach with a Taylor Series Linearisation. PML is a form of weighted estimation that replaces the usual sample statistics usually used with a weighted version. Taylor Series Linearisation provides a correction to the standard errors."
  },
  {
    "objectID": "06_survey.html#what-is-complex-survey-data",
    "href": "06_survey.html#what-is-complex-survey-data",
    "title": "6. Complex Survey Data",
    "section": "",
    "text": "Complex survey data refers to data that have been collected according to an explicit sampling scheme that deviates from a simple random sample (where everyone in a target population has the same chance of being selected into a sample). This might be done to save data collection resources, or to make sure that there are sufficient numbers of a small group in the sample.\nDue to their advantages, complex sampling designs are very common in large-scale survey data. These datasets usually provide variables relating to their complex sampling designs so that users can take them into account in their analyses.\nDatasets might include stratification, clustering, and/or sampling weights (sometimes called ‘survey’ or ‘design weights’) which correspond to the different characteristics of complex sampling. Stratification is when the population is divided into relatively homogeneous groups and a number of pre-determined units is sampled from each. Stratification variables indicate to which stratum a case belongs. For example…..[XXX]. Clustering is when units are sampled in ‘clumps’, for example, rather than randomly sampling households within a whole large area, people within a subset of neighbourhoods within that area might be sampled. Clustering variables thus indicate to which cluster a case belongs. Sampling weights reflect the unequal chances of people being selected into a sample. Cases that had a high probability of selection will have low weights and people who had a low probability of selection will have have high weights. This means that when it comes to the analysis, under-represented groups (relative to the population) will be correspondingly up-weighted and over-represented groups will be down-weighted.\nIf a dataset has been collected using a complex survey sampling design it is important to take this into account in analyses of those datasets if the research questions relate to the underlying population that the sample was taken from. For example, ignoring clustering will tend to give standard errors that are too small and will inflate statistical significance. Ignoring sampling weights can result in the wrong values for the treatment estimates.\nDigiCAT currently implements a ‘design-based’ approach to dealing with complex survey data. This means that it uses information about how the data were sampled (I.e., any sampling, stratification, and clustering variables) when doing the analysis. It specifically uses a ‘pseudomaximum likelihood estimation’ (PML) approach with a Taylor Series Linearisation. PML is a form of weighted estimation that replaces the usual sample statistics usually used with a weighted version. Taylor Series Linearisation provides a correction to the standard errors."
  },
  {
    "objectID": "06_survey.html#complex-survey-data-in-psm",
    "href": "06_survey.html#complex-survey-data-in-psm",
    "title": "6. Complex Survey Data",
    "section": "Complex survey data in PSM",
    "text": "Complex survey data in PSM\nWhen estimating the population ATT (PATT) it is important to take the survey design into account in the outcome model. This means that any stratification, clustering, and weighting variables must feature in the linear regression outcome model that comes at the end of the PSM workflow. It does so using the PML and Taylor Series Linearisation solution mentioned above.\nPrevious research has been less clear on whether complex survey design variables need to be taken into account at earlier stages of the PSM workflow, namely when estimating the propensity model and when assessing balance. Some authors have made conceptual arguments against fitting design-adjusted propensity models and instead including survey weights as a covariate (DuGoff et al., 2014). This is based on the idea that propensity scores are inherently concerns with samples rather than populations and so there is no need to generalise them to a population. When researchers have used simulation studies to explore this issue they have found inconsistent evidence and this has led some to propose that it doesn’t really make an important difference (Ridgeway et al. 2015; Lenis et al., 2019; Austin et al., 2018). However, one quite comprehensive simulation study did find that for continuous outcomes like the ones that can be studied with DigiCAT there were benefits to fitting a design-adjusted model for propensity score estimation and for taking complex survey variables into account when assessing balance (Austin et al., 2018). This was further supported by another comprehensive simulation study (Lenis et al., 2019 ). For this reason, in DigiCAT if users supply complex survey variables, these are also taken into account when estimating the propensity score with a logistic regression and when assessing matching variable balance. The way that they are taken into account in the logistic regression is very similar to the way they feature in the linear regression outcome model. [gloss details]. The way they are taken into account in assessing balance is [brief detail]."
  },
  {
    "objectID": "06_survey.html#missing-data-and-complex-survey-data",
    "href": "06_survey.html#missing-data-and-complex-survey-data",
    "title": "6. Complex Survey Data",
    "section": "Missing data and complex survey data",
    "text": "Missing data and complex survey data\n\nApproaches in DigiCAT [Aja]\n\nPML\n\nCombining missing data and complex survey data\n\nMI\nWeighting\n\nWorked example of PSM with social media & mental health"
  },
  {
    "objectID": "07_further.html",
    "href": "07_further.html",
    "title": "Acknowledgements & Further reading",
    "section": "",
    "text": "We are grateful to the Wellcome Trust for funding the Discovery and Prototyping phases of the development of DigiCAT. Our thanks also go to the many who contributed to DigiCAT in various ways. We are grateful to the young person advisory groups (YPAGs) who provided invaluable insights and help guide the direction of DigiCAT, to the users who responded to our user survey and to the group members who provided feedback on iterations of the tool."
  },
  {
    "objectID": "07_further.html#acknowledgements",
    "href": "07_further.html#acknowledgements",
    "title": "Acknowledgements & Further reading",
    "section": "",
    "text": "We are grateful to the Wellcome Trust for funding the Discovery and Prototyping phases of the development of DigiCAT. Our thanks also go to the many who contributed to DigiCAT in various ways. We are grateful to the young person advisory groups (YPAGs) who provided invaluable insights and help guide the direction of DigiCAT, to the users who responded to our user survey and to the group members who provided feedback on iterations of the tool."
  },
  {
    "objectID": "07_further.html#further-reading",
    "href": "07_further.html#further-reading",
    "title": "Acknowledgements & Further reading",
    "section": "Further Reading",
    "text": "Further Reading\nAccessible introductions to causal inference\n\nPearl’s Primer on causal inference\n\nMore on counterfactual analysis\n\n[recent book that included the chapter on NPB]\n[matchIT /other sources on doing it in R]\nCaliendo, M., & Kopeinig, S. (2008). Some practical guidance for the implementation of propensity score matching. Journal of economic surveys, 22(1), 31-72.\n\nAdvanced topics in counterfactual analysis?\n\n[counterfactual analysis with SEM?]\n[causal mediation?]\n[heterogeneous treatment effects?]\n\nMore on missing data\n\nNguyen, C. D., J. B. Carlin, and K. J. Lee. 2017. “Model Checking in Multiple Imputation: An Overview and Case Study.” Emerging Themes in Epidemiology 14 (1): 8.\n\nMore on counterfactual analysis and complex survey designs\n\nAustin, Peter C., Nathaniel Jembere, and Maria Chiu. 2016. “Propensity Score Matching and Complex Surveys.” Statistical Methods in Medical Research 27 (4): 1240–57. https://doi.org/10.1177/0962280216658920.\n\nLenis, David, Trang Quynh Nguyen, Nianbo Dong, and Elizabeth A. Stuart. 2019. “It’s All about Balance: Propensity Score Matching in the Context of Complex Survey Data.” Biostatistics 20 (1): 147–63. https://doi.org/10.1093/biostatistics/kxx063.\n\nMore on machine learning ensemble methods\n\nMcCaffrey, D. F., Ridgeway, G., & Morral, A. R. (2004). Propensity Score Estimation With Boosted Regression for Evaluating Causal Effects in Observational Studies. Psychological Methods, 9(4), 403–425.\n\nExamples of papers using counterfactual analysis\n\n[one per major approach implemented in DigiCAT or our own papers that implement multiple approaches]"
  },
  {
    "objectID": "csstests.html",
    "href": "csstests.html",
    "title": "Tests",
    "section": "",
    "text": "learning obj\n\n\nimportant\n\n\nsticky\n\n\n\n\n\nr tips\n\n\nstatbox\n\n\ninterpretation interpretation interpretation\n\n\nQuestion\n\n\nquestion\nwhat is your name?\nwhat is your favourite colour?\n\n\n\n\n Solution \n\n\nsolution\nhello\n\n2+2\n\n[1] 4\n\n\n\n\n\n\n Optional hello my optional friend\n\n\nit’s nice to see you again\n\n\n\n\n\nthis is not a panel\n\n\nthis is a panel\n\n\nthis is a panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DigiCAT Tutorial Pages",
    "section": "",
    "text": "Welcome to the tutorial pages for DigiCAT! The tutorials provide information on what counterfactual analysis is, why it can be useful, and how to do it using DigiCAT. These pages are designed to be accessible to those with minimal experience of counterfactual analysis. As such, where we provide technical details, we have also endeavoured to provide a translation; a ‘what does this mean?’. If after reading through the tutorials you crave more information on counterfactual analysis you can check out the further reading."
  },
  {
    "objectID": "index.html#overview-of-tutorials",
    "href": "index.html#overview-of-tutorials",
    "title": "DigiCAT Tutorial Pages",
    "section": "",
    "text": "Welcome to the tutorial pages for DigiCAT! The tutorials provide information on what counterfactual analysis is, why it can be useful, and how to do it using DigiCAT. These pages are designed to be accessible to those with minimal experience of counterfactual analysis. As such, where we provide technical details, we have also endeavoured to provide a translation; a ‘what does this mean?’. If after reading through the tutorials you crave more information on counterfactual analysis you can check out the further reading."
  },
  {
    "objectID": "index.html#how-to-engage-with-tutorials",
    "href": "index.html#how-to-engage-with-tutorials",
    "title": "DigiCAT Tutorial Pages",
    "section": "How to engage with tutorials",
    "text": "How to engage with tutorials\nThe tutorials are structured such that users can read only a sub-section of the sections relevant for their analysis and tailored to their level of background knowledge. If you are a first-time user of counterfactual analysis we recommend reading sections X-Y. If you need some help choosing a counterfactual analysis approach, we recommend reading section X. If you are already confident in your knowledge of counterfactual analysis and know which approach you would like to implement you can go directly to the relevant tutorial page for information on how it is implemented in DigiCAT."
  }
]